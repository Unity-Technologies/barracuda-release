#pragma kernel Conv2D_NHWC CHANNELS_FIRST=0
#pragma kernel Conv2D_NCHW CHANNELS_FIRST=1
#pragma kernel Conv2D_RegisterBlock4x2_NHWC CHANNELS_FIRST=0
#pragma kernel Conv2D_RegisterBlock4x2_NCHW CHANNELS_FIRST=1
//#pragma kernel Conv2D_L1Cached64_RegisterBlock4x4_NHWC CHANNELS_FIRST=0
//#pragma kernel Conv2D_L1Cached64_RegisterBlock4x4_NCHW CHANNELS_FIRST=1
//#pragma kernel Conv2D_L1Cached32_RegisterBlock4x4_NHWC CHANNELS_FIRST=0
//#pragma kernel Conv2D_L1Cached32_RegisterBlock4x4_NCHW CHANNELS_FIRST=1

//R4x4_64k
#pragma kernel Conv2DKernelKxK_T16x16_R4x4_NHWC                   CHANNELS_FIRST=0 BLOCK_SIZE=4                                                 SUFFIX=KernelKxK_T16x16_R
#pragma kernel Conv2DKernelKxK_StrictC16K64_T16x16_R4x4_NHWC      CHANNELS_FIRST=0 BLOCK_SIZE=4                  STRICT_CHANNELS=1              SUFFIX=KernelKxK_StrictC16K64_T16x16_R
#pragma kernel Conv2DKernel1x1_StrictC16K64_T16x16_R4x4_NHWC      CHANNELS_FIRST=0 BLOCK_SIZE=4 KERNEL_1x1=1     STRICT_CHANNELS=1              SUFFIX=Kernel1x1_StrictC16K64_T16x16_R
//TODO support CHW
//#pragma kernel Conv2DKernelKxK_T16x16_R4x4_NCHW                   CHANNELS_FIRST=1 BLOCK_SIZE=4                                                 SUFFIX=KernelKxK_T16x16_R
//#pragma kernel Conv2DKernelKxK_StrictC16K64_T16x16_R4x4_NCHW      CHANNELS_FIRST=1 BLOCK_SIZE=4                  STRICT_CHANNELS=1              SUFFIX=KernelKxK_StrictC16K64_T16x16_R
//#pragma kernel Conv2DKernel1x1_StrictC16K64_T16x16_R4x4_NCHW      CHANNELS_FIRST=1 BLOCK_SIZE=4 KERNEL_1x1=1     STRICT_CHANNELS=1              SUFFIX=Kernel1x1_StrictC16K64_T16x16_R
//R8x8_64k
#pragma kernel Conv2DKernelKxK_StrictC16StrictK64_T8x8_R8x8_NHWC  CHANNELS_FIRST=0 BLOCK_SIZE=8 KERNEL_PER_TG=64 STRICT_CHANNELS=1              SUFFIX=KernelKxK_StrictC16StrictK64_T8x8_R
#pragma kernel Conv2DKernelKxK_StrictC16StrictK64_T8x8_R8x8_NCHW  CHANNELS_FIRST=1 BLOCK_SIZE=8 KERNEL_PER_TG=64 STRICT_CHANNELS=1              SUFFIX=KernelKxK_StrictC16StrictK64_T8x8_R
#pragma kernel Conv2DKernelKxK_StrictC16LaxK64_T8x8_R8x8_NHWC  CHANNELS_FIRST=0 BLOCK_SIZE=8 KERNEL_PER_TG=64 STRICT_CHANNELS=1 LAX_KERNEL=1    SUFFIX=KernelKxK_StrictC16LaxK64_T8x8_R
#pragma kernel Conv2DKernelKxK_StrictC16LaxK64_T8x8_R8x8_NCHW  CHANNELS_FIRST=1 BLOCK_SIZE=8 KERNEL_PER_TG=64 STRICT_CHANNELS=1 LAX_KERNEL=1    SUFFIX=KernelKxK_StrictC16LaxK64_T8x8_R
//R8x8_16k
#pragma kernel Conv2DKernelKxK_StrictC4StrictK16_T2x32_R8x8_NHWC  CHANNELS_FIRST=0 BLOCK_SIZE=8 KERNEL_PER_TG=16 STRICT_CHANNELS=1              SUFFIX=KernelKxK_StrictC4StrictK16_T2x32_R
#pragma kernel Conv2DKernelKxK_StrictC4StrictK16_T2x32_R8x8_NCHW  CHANNELS_FIRST=1 BLOCK_SIZE=8 KERNEL_PER_TG=16 STRICT_CHANNELS=1              SUFFIX=KernelKxK_StrictC4StrictK16_T2x32_R
#pragma kernel Conv2DKernelKxK_LaxC4StrictK16_T2x32_R8x8_NHWC     CHANNELS_FIRST=0 BLOCK_SIZE=8 KERNEL_PER_TG=16                                SUFFIX=KernelKxK_LaxC4StrictK16_T2x32_R
#pragma kernel Conv2DKernelKxK_LaxC4StrictK16_T2x32_R8x8_NCHW     CHANNELS_FIRST=1 BLOCK_SIZE=8 KERNEL_PER_TG=16                                SUFFIX=KernelKxK_LaxC4StrictK16_T2x32_R
#pragma kernel Conv2DKernelKxK_StrictC4LaxK16_T2x32_R8x8_NHWC     CHANNELS_FIRST=0 BLOCK_SIZE=8 KERNEL_PER_TG=16 STRICT_CHANNELS=1 LAX_KERNEL=1 SUFFIX=KernelKxK_StrictC4LaxK16_T2x32_R
#pragma kernel Conv2DKernelKxK_StrictC4LaxK16_T2x32_R8x8_NCHW     CHANNELS_FIRST=1 BLOCK_SIZE=8 KERNEL_PER_TG=16 STRICT_CHANNELS=1 LAX_KERNEL=1 SUFFIX=KernelKxK_StrictC4LaxK16_T2x32_R

#pragma kernel DepthwiseConv2D_NHWC CHANNELS_FIRST=0
#pragma kernel DepthwiseConv2D_NCHW CHANNELS_FIRST=1

#pragma kernel Conv2DTrans_NHWC CHANNELS_FIRST=0
#pragma kernel Conv2DTrans_NCHW CHANNELS_FIRST=1

//Tested 2x2, 3x3 and 5x5 kernels with groupsize [8,8], [8,16], [16,16] and [16,32] (this one not in 5x5 as it does not fit in 32k)
//k=5x5 t=[16,16] fast consistently faster or equal to other configuration both on AMDVega and RTX2080 (tested with kernel size 2x2x32x32, input size 128x128x32)
//however this configuration is quite LDS bound performance profile might be very different on hardware without on chip LDS. This is especially true for smaller kernel
//as a lot of LDS will be reserved but not used, reducing the amount of cache used.
#pragma kernel Conv2DTrans_KernelCached_K5x5_T16x16_NHWC  CHANNELS_FIRST=0 MAX_KERNEL_SIZE=5 GROUP_SIZE_X=16 GROUP_SIZE_Y=16
#pragma kernel Conv2DTrans_KernelCached_K5x5_T16x16_NCHW  CHANNELS_FIRST=1 MAX_KERNEL_SIZE=5 GROUP_SIZE_X=16 GROUP_SIZE_Y=16

#pragma kernel Conv2DTransFlipKernel
#pragma kernel Conv2DTransPadFill_NHWC CHANNELS_FIRST=0
#pragma kernel Conv2DTransPadFill_NCHW CHANNELS_FIRST=1

#pragma kernel KernelWinograd_3x3
#pragma kernel Conv2DWinograd_2x2_3x3_NHWC CHANNELS_FIRST=0
#pragma kernel Conv2DWinograd_2x2_3x3_NCHW CHANNELS_FIRST=1

#include "Tensor.cginc"

TENSOR_DECL(X)
TENSOR_DECL(K)
TENSOR_DECL(B)
TENSOR_DECL(WBK)
TENSOR_DECL_RW(O)

uint4 _Pad;
uint4 _Stride;

#define DEBUG_CHECK_BOUNDS 0

// Conv2DBlock64x64_4x4 + index optimizations
//        T
//      -1|0             -1|0
// 16: 142|142ms        144|155ms

float ffma(float a, float b, float c) { return dot(float2(a,c), float2(b,1)); }

#if CHANNELS_FIRST
    #define FUNC_NAME(KERNEL, SUFFIX, SIZE) KERNEL##SUFFIX##SIZE##x##SIZE##_NCHW
    #define CACHE_NAME(KERNEL, SUFFIX, SIZE, TENSOR) KERNEL##SUFFIX##SIZE##x##SIZE##_Cache_##TENSOR##_NCHW
#else
    #define FUNC_NAME(KERNEL, SUFFIX, SIZE) KERNEL##SUFFIX##SIZE##x##SIZE##_NHWC
    #define CACHE_NAME(KERNEL, SUFFIX, SIZE, TENSOR) KERNEL##SUFFIX##SIZE##x##SIZE##_Cache_##TENSOR##_NHWC
#endif

#define KERNEL_NAME Conv2D

#if BLOCK_SIZE == 8
#if KERNEL_PER_TG == 64

#if CHANNELS_FIRST
    //NCHW
    #define CACHE_DEPTH 8                      // Profiled as the fastest to avoid 'tail' of inner loops with occupancy 1 at end of dispatch.
    #define CACHE_WIDTH_W_PAD 1
    #define NUM_DDR_LOAD_PER_LOOP CACHE_DEPTH  // Not needed for NCHW
    #define SHUFFLE_FOR_COALESCED_LOAD 0       // Not needed for NCHW
    #define SHUFFLE_FOR_COALESCED_STORE 1
#else
    //NHWC
    #define CACHE_DEPTH 16                     // Only supported value
    #define CACHE_WIDTH_W_PAD 0                // Only supported value
    #define NUM_DDR_LOAD_PER_LOOP 8            // <=8 required to lower register pressure for NHWC for occupancy of 2.
    #define SHUFFLE_FOR_COALESCED_LOAD 1
    #define SHUFFLE_FOR_COALESCED_STORE 1
#endif
#define CACHE_WIDTH_X 64                       // Only supported value
#define CACHE_WIDTH_W (64+CACHE_WIDTH_W_PAD)   // Only supported value

#if SHUFFLE_FOR_COALESCED_STORE
    //A TG output [64pixels,64channels] = 4096 values, We will write two time 2048 values to DDR (8k LDS).
    groupshared float CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, LDS)[2048];
#else
    groupshared float CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, LDS)[CACHE_DEPTH*(CACHE_WIDTH_X+CACHE_WIDTH_W)];
#endif

[numthreads(8,8,1)]
void FUNC_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE)(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint threadIndex : SV_GroupIndex)
{
    //This kernel assume the following:
    //Input:
    //  BatchCount == 1 (and thus layout is in fact CHW or HWC)
    //  C % CACHE_DEPTH==0 <-- only if STRICT_CHANNELS==1
    //Ouput:
    //  W%4==0 <-- only if CHANNELS_FIRST==1
    //Kernel:
    //  K%64==0 <-- only if LAX_KERNEL=0 else K%16==0 is required
    DISPATCH_ARGS(K.kernelCount, O.width * O.height, 1);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);
    #define LDS_ CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, LDS)
    #define X_OFFSET 0
    #define W_OFFSET CACHE_DEPTH*CACHE_WIDTH_X

    //Per thread group (scalar registers)
    uint tg_NumChannels = X.channels;
    uint tg_WidthX  = X.width;
    uint tg_HeightX = X.height;
    uint tg_WidthO  = O.width;
    uint tg_HeightO = O.height;
    uint tg_NumKernels = K.channels;
    uint tg_NumInputPixels = tg_WidthX*tg_HeightX;
    uint tg_NumOuputPixels = tg_WidthO*tg_HeightO;
    uint tg_KernelSpatialStride = tg_NumKernels*tg_NumChannels;
    uint tg_KernelBaseId = groupID.x * CACHE_WIDTH_X;
    uint tg_OutputPixelBaseId = groupID.y * CACHE_WIDTH_X;
    uint tg_kernelSpatialOffset = 0;

    //8x8 block, 8 kernels by 8 pixels
    //**********************************
    //* Kernel Ids  *  0  1  2  3  ...
    //**********************************
    //              *  ThreadIds
    // Pixel Ids  0 *  0  1  2  3 ...
    //            1 *  8  9 10 11 ...
    //            2 * 16 17 18 19 ...
    //            3 * 32 33 34 35 ...
    //            ... ...
    float dstA[BLOCK_SIZE*BLOCK_SIZE];

    //Load Bias [K] int dstA [Kernels, Pixels]
    uint tg_kId;
    uint tg_pId;
    [unroll] for (tg_pId = 0; tg_pId < BLOCK_SIZE; ++tg_pId)
        [unroll] for (tg_kId = 0; tg_kId < BLOCK_SIZE; ++tg_kId)
            dstA[tg_pId*BLOCK_SIZE+tg_kId] = B.FastGet(tg_KernelBaseId + groupThreadID.x * BLOCK_SIZE + tg_kId);

    //Loop spatialy on kernels
    for (uint tg_Dy = 0; tg_Dy < K.GetKernelHeight(); tg_Dy++)
    {
        for (uint tg_Dx = 0; tg_Dx < K.GetKernelWidth(); tg_Dx++)
        {
            for (uint tg_ChannelOffset = 0; tg_ChannelOffset < tg_NumChannels; tg_ChannelOffset += CACHE_DEPTH)
            {
                uint tg_CacheLoadDynIdx = 0;
                //Load from DDR to LDS: (64 weight + 64 pixel) * CACHE_DEPTH => 512Bytes * CACHE_DEPTH.
                //Storing in registers to avoid sync inside the loop.
                #if NUM_DDR_LOAD_PER_LOOP != CACHE_DEPTH
                for (; tg_CacheLoadDynIdx < CACHE_DEPTH/NUM_DDR_LOAD_PER_LOOP; ++tg_CacheLoadDynIdx)
                #endif
                {
                    //Explicit register declaration as [unroll] won't unroll properly otherwise and introduce sync points.
                    float tempW[NUM_DDR_LOAD_PER_LOOP];
                    float tempX[NUM_DDR_LOAD_PER_LOOP];
                    uint tg_regCacheLoadIdx;
                    [unroll] for (tg_regCacheLoadIdx = 0; tg_regCacheLoadIdx < NUM_DDR_LOAD_PER_LOOP; ++tg_regCacheLoadIdx)
                    {
                        uint tg_CacheLoadIdx = tg_CacheLoadDynIdx * NUM_DDR_LOAD_PER_LOOP + tg_regCacheLoadIdx;
                        //K stored as HWCK, threadgroup is loading 64 kernels at a time to LDS in a linear fashion.
                        //HW from tg_kernelSpatialOffset
                        //C from tg_ChannelOffset+tg_CacheLoadIdx
                        //K from tg_KernelBaseId (for TG) + threadIndex ([0-63])
                        uint tg_KernelReadOffset = tg_kernelSpatialOffset + tg_NumKernels*(tg_ChannelOffset+tg_CacheLoadIdx) + tg_KernelBaseId;
                        uint kernelReadOffset = tg_KernelReadOffset + threadIndex;
                        #if !STRICT_CHANNELS || LAX_KERNEL
                            kernelReadOffset = min(kernelReadOffset, K.GetLength()-1);
                        #endif
                        tempW[tg_regCacheLoadIdx] = K.FastGet(kernelReadOffset);

                        //Compute input position and mask.
                        #if SHUFFLE_FOR_COALESCED_LOAD
                            //64 Reads per TG per loop -> 4 pixels x 16 channels across threads -> good for NHWC.
                            //IMPORTANT : For register pressure reason -> it is assumed that tg_WidthO % 4 == 0, so we know all
                            //pixels for a given TG+loop are on the same row and thus we can compute Y mask/pos using scalar registers.
                            uint cacheChannelId = threadIndex % 16;
                            int tg_outputPixelBaseId = tg_OutputPixelBaseId + tg_CacheLoadIdx * 4;
                            int2 tg_ouputPixelsBaseCoord = int2(tg_outputPixelBaseId % tg_WidthO, tg_outputPixelBaseId / tg_WidthO);
                            int2 tg_inputPixelsBaseCoord = tg_ouputPixelsBaseCoord * _Stride.xy - _Pad.xy + int2(tg_Dx, tg_Dy);
                            bool tg_inputPixelsYMask = (tg_inputPixelsBaseCoord.y >= 0) && (tg_inputPixelsBaseCoord.y < (int)tg_HeightX);
                            int inputPixelXCoord = (threadIndex / 16) * _Stride.x + tg_inputPixelsBaseCoord.x;
                            bool inputPixelMask = tg_inputPixelsYMask && (inputPixelXCoord >= 0) && (inputPixelXCoord < (int)tg_WidthX);
                            int2 inputPixelCoords = int2(inputPixelXCoord, tg_inputPixelsBaseCoord.y);//.y is scalar
                        #else
                            //64 Reads per TG per loop -> 64 pixels across threads -> good for NCHW.
                            uint cacheChannelId = tg_CacheLoadIdx;//scalar in that code path.
                            int outputPixelBaseId = tg_OutputPixelBaseId + threadIndex;
                            int2 outputPixelCoords = int2(outputPixelBaseId % tg_WidthO, outputPixelBaseId / tg_WidthO);
                            int2 inputPixelCoords = outputPixelCoords * _Stride.xy - _Pad.xy + int2(tg_Dx, tg_Dy);
                            bool inputPixelMask = all( (inputPixelCoords >= 0) && (inputPixelCoords < float2(tg_WidthX, tg_HeightX)) );
                        #endif
                        int inputPixelId = inputPixelCoords.y * tg_WidthX + inputPixelCoords.x;
                        uint inputChannelId = tg_ChannelOffset + cacheChannelId;
                        bool inputChannelMask = inputChannelId < tg_NumChannels;
                        #if STRICT_CHANNELS
                            inputChannelMask = true;
                        #endif
                        #if CHANNELS_FIRST
                            uint pixelReadOffset = tg_NumInputPixels * inputChannelId + inputPixelId;
                        #else
                            uint pixelReadOffset = tg_NumChannels * inputPixelId + inputChannelId;
                        #endif
                        tempX[tg_regCacheLoadIdx] = X.MaskedGet(inputPixelMask && inputChannelMask, pixelReadOffset);
                    }

                    [unroll] for (tg_regCacheLoadIdx = 0; tg_regCacheLoadIdx < NUM_DDR_LOAD_PER_LOOP; ++tg_regCacheLoadIdx)
                    {
                        uint tg_CacheLoadIdx = tg_CacheLoadDynIdx * NUM_DDR_LOAD_PER_LOOP + tg_regCacheLoadIdx;
                        #if SHUFFLE_FOR_COALESCED_LOAD
                            uint cachePixelId = tg_CacheLoadIdx * 4 + threadIndex / 16;
                            uint cacheChannelId = threadIndex % 16;
                        #else
                            uint cachePixelId = threadIndex;
                            uint cacheChannelId = tg_CacheLoadIdx;//scalar in that code path.
                        #endif
                        uint weightWriteIndex = (threadIndex>31)?threadIndex+CACHE_WIDTH_W_PAD:threadIndex;
                        LDS_[ W_OFFSET + tg_CacheLoadIdx*CACHE_WIDTH_W + weightWriteIndex ] = tempW[tg_regCacheLoadIdx];
                        LDS_[ X_OFFSET + cacheChannelId*CACHE_WIDTH_X + cachePixelId ] = tempX[tg_regCacheLoadIdx];
                    }
                }

                GroupMemoryBarrierWithGroupSync();

                //Inner loop
                uint ptrX = groupThreadID.y*BLOCK_SIZE + X_OFFSET;
                uint ptrW = groupThreadID.x*BLOCK_SIZE + W_OFFSET;
                ptrW += (groupThreadID.x*BLOCK_SIZE>31)?CACHE_WIDTH_W_PAD:0;
                for (uint tg_CacheExecuteIdx = 0; tg_CacheExecuteIdx < CACHE_DEPTH; ++tg_CacheExecuteIdx)
                {
                    //Load LDS -> registers
                    float colOfX[BLOCK_SIZE];
                    float rowOfW[BLOCK_SIZE];
                    uint tg_q;
                    [unroll] for (tg_q = 0; tg_q < BLOCK_SIZE; ++tg_q)
                        colOfX[tg_q] = LDS_[ptrX + tg_q];
                    [unroll] for (tg_q = 0; tg_q < BLOCK_SIZE; ++tg_q)
                        rowOfW[tg_q] = LDS_[ptrW + tg_q];

                    ptrX += CACHE_WIDTH_X;
                    ptrW += CACHE_WIDTH_W;

                    //Mads 8 pixels by 8 kernels matmul style --> 64 mads
                    [unroll] for (uint tg_X = 0; tg_X < BLOCK_SIZE; ++tg_X)
                        [unroll] for (uint tg_W = 0; tg_W < BLOCK_SIZE; ++tg_W)
                            dstA[tg_X*BLOCK_SIZE+tg_W] = ffma(colOfX[tg_X], rowOfW[tg_W], dstA[tg_X*BLOCK_SIZE+tg_W]);
                }

                GroupMemoryBarrierWithGroupSync();
            }

            tg_kernelSpatialOffset += tg_KernelSpatialStride;
        }
    }

    #if SHUFFLE_FOR_COALESCED_STORE
        //-----------------------------------------------------
        //Use LDS to shuffle TG registers into coalesced writes
        //-----------------------------------------------------
        //A TG output [64pixels,64channels] = 4096 values, We will process [32,64] values at a time per TG.
        #if CHANNELS_FIRST
            //NCHW
            for (uint tg_registerChannelOffset = 0; tg_registerChannelOffset < BLOCK_SIZE; tg_registerChannelOffset += 4)
            {
                //Store 8 pixels x 4 channels per threads to LDS.
                uint ldsRowOffset = groupThreadID.x * 4;
                uint ldsPixelOffset = groupThreadID.y * BLOCK_SIZE;
                [unroll] for (tg_kId = 0; tg_kId < 4; ++tg_kId)
                    [unroll] for (tg_pId = 0; tg_pId < BLOCK_SIZE; ++tg_pId)
                    {
                        //To avoid bank conflict store in 32 groups [8pixelsGroups,4channelsGroups] each group contain 64 values [8pixels,8kernels] for a total of 2048 values [64pixels,32channels]
                        uint ldsOffsetOfGroup = CACHE_WIDTH_X * (tg_kId*BLOCK_SIZE+tg_pId);//64 * ([0,3]*8+[0,7]) = [0,1984]
                        LDS_[ldsOffsetOfGroup + threadIndex] = dstA[BLOCK_SIZE * tg_pId + (tg_registerChannelOffset + tg_kId)];
                    }

                GroupMemoryBarrierWithGroupSync();

                //We have a buffers of [64pixels,32channels] floats, each thread will store [1pixels,32channels] so a threadgroup is storing 64 pixels at a time to DDR in a linear fashion.
                uint readPixelId = threadIndex;
                uint writePixelId = tg_OutputPixelBaseId + readPixelId;

                #define WRITE_8CHANNELS_IF_POSSIBLE(groupID) \
                tg_ddrChannelGroupBaseId[groupID] = tg_KernelBaseId + 16 * groupID; \
                if (tg_ddrChannelGroupBaseId[groupID] < tg_NumKernels) \
                { \
                    [unroll] for (tg_kId = groupID*8; tg_kId < 8*(groupID+1); ++tg_kId) \
                    { \
                        uint tg_kIdOfGroup = tg_kId % 4; \
                        uint pIdOfGroup = readPixelId % BLOCK_SIZE; \
                        uint ldsOffsetOfGroup = CACHE_WIDTH_X * (tg_kIdOfGroup * BLOCK_SIZE + pIdOfGroup); \
                        uint tg_kIdInGroup = (tg_kId - tg_kIdOfGroup) / 4; \
                        uint pIdInGroup = (readPixelId - pIdOfGroup) / BLOCK_SIZE; \
                        uint ldsOffsetInGroup = pIdInGroup * BLOCK_SIZE + tg_kIdInGroup; \
                        uint readIndex = ldsOffsetOfGroup + ldsOffsetInGroup; \
                        uint writeChannelId = tg_KernelBaseId + tg_kId%4 + (tg_kId/4)*BLOCK_SIZE + tg_registerChannelOffset; \
                        uint writeIndex = O.width * O.height * writeChannelId + writePixelId; \
                        O.FastSetWithActivation(writeIndex, LDS_[readIndex]); \
                    } \
                }

                if (writePixelId < tg_NumOuputPixels)
                {
                    #if LAX_KERNEL
                        uint tg_ddrChannelGroupBaseId[4];
                        WRITE_8CHANNELS_IF_POSSIBLE(0);
                        WRITE_8CHANNELS_IF_POSSIBLE(1);
                        WRITE_8CHANNELS_IF_POSSIBLE(2);
                        WRITE_8CHANNELS_IF_POSSIBLE(3);
                    #else
                        [unroll] for (tg_kId = 0; tg_kId < 32; ++tg_kId)
                        {
                            //Find LDS group to read from
                            uint tg_kIdOfGroup = tg_kId % 4;//[0,3] kernelsGroups
                            uint pIdOfGroup = readPixelId % BLOCK_SIZE;//[0,7] pixelsGroups
                            uint ldsOffsetOfGroup = CACHE_WIDTH_X * (tg_kIdOfGroup * BLOCK_SIZE + pIdOfGroup);//CACHE_WIDTH_X * ([0,3]*8+[0,7]) = [0,1984]
                            //Find index inside that group
                            uint tg_kIdInGroup = (tg_kId - tg_kIdOfGroup) / 4;//[0,7] kernels
                            uint pIdInGroup = (readPixelId - pIdOfGroup) / BLOCK_SIZE;//[0,7] pixels
                            uint ldsOffsetInGroup = pIdInGroup * BLOCK_SIZE + tg_kIdInGroup;//[0,7]*8+[0,7] = [0,63]
                            //load from LDS and store to DDR
                            uint readIndex = ldsOffsetOfGroup + ldsOffsetInGroup;//[0,2047]
                            uint writeChannelId = tg_KernelBaseId + tg_kId%4 + (tg_kId/4)*BLOCK_SIZE + tg_registerChannelOffset;
                            uint writeIndex = O.width * O.height * writeChannelId + writePixelId;
                            //TODO Still some bank conflict here, an option would be to pad LDS but need more loop then (as already have 8k LDS with two loop).
                            O.FastSetWithActivation(writeIndex, LDS_[readIndex]);
                        }
                    #endif
                }

                GroupMemoryBarrierWithGroupSync();
            }
        #else
            //NHWC
            for (uint tg_registerPixelOffset = 0; tg_registerPixelOffset < BLOCK_SIZE; tg_registerPixelOffset += 4)
            {
                //Store 4 pixels x 8 channels per threads to LDS.
                uint ldsRowOffset = groupThreadID.y * 4;
                uint ldsChannelOffset = groupThreadID.x * BLOCK_SIZE;
                [unroll] for (tg_pId = 0; tg_pId < 4; ++tg_pId)
                    [unroll] for (tg_kId = 0; tg_kId < BLOCK_SIZE; ++tg_kId)
                    {
                        //TODO check for bank conflict here, probably need to swizzle the writes per thread
                        LDS_[CACHE_WIDTH_X * (ldsRowOffset + tg_pId) + ldsChannelOffset + tg_kId] = dstA[BLOCK_SIZE * (tg_registerPixelOffset + tg_pId) + tg_kId];
                    }

                GroupMemoryBarrierWithGroupSync();

                //We have a buffers of [32pixels,64channels] floats, each thread will store [32pixels,1channels] so a threadgroup is storing 64 kernels at a time to DDR in a linear fashion.
                uint writeChannelId = tg_KernelBaseId + threadIndex;
                uint tg_writeLoopBaseId = tg_OutputPixelBaseId + tg_registerPixelOffset;
                uint tg_ddrPixelGroupBaseId[8];

                #if LAX_KERNEL
                    bool canWriteChannel = (writeChannelId < tg_NumKernels);
                #else
                    bool canWriteChannel = true;
                #endif

                //Ok as we enforce W%4==0 thus W*H%4==0 also.
                //Using a Macro as [unroll] on loop(groupID) won't unroll properly and thus introduce LDS/DDR sync points.
                #define WRITE_4PIXELS_IF_POSSIBLE(groupID) \
                tg_ddrPixelGroupBaseId[groupID]= tg_writeLoopBaseId + BLOCK_SIZE * groupID; \
                if ((tg_ddrPixelGroupBaseId[groupID] < tg_NumOuputPixels) && canWriteChannel)\
                { \
                    [unroll] for (tg_pId = 0; tg_pId < 4; ++tg_pId) \
                        O.FastSetWithActivation(tg_NumKernels * (tg_ddrPixelGroupBaseId[groupID]+tg_pId) + writeChannelId, LDS_[CACHE_WIDTH_X * (groupID * 4 + tg_pId) + threadIndex]); \
                }
                WRITE_4PIXELS_IF_POSSIBLE(0);
                WRITE_4PIXELS_IF_POSSIBLE(1);
                WRITE_4PIXELS_IF_POSSIBLE(2);
                WRITE_4PIXELS_IF_POSSIBLE(3);
                WRITE_4PIXELS_IF_POSSIBLE(4);
                WRITE_4PIXELS_IF_POSSIBLE(5);
                WRITE_4PIXELS_IF_POSSIBLE(6);
                WRITE_4PIXELS_IF_POSSIBLE(7);
                #undef WRITE_PIXEL_GROUP_IF_POSSIBLE

                GroupMemoryBarrierWithGroupSync();
            }
        #endif //CHANNELS_FIRST
    #else
		//-------------------------------
		//Directly store registers to DDR
		//-------------------------------
		//B does not require an offset as size == 1
		//C from tg_KernelBaseId, groupThreadID.x and tg_kId
		//HW from tg_OutputPixelBaseId, groupThreadID.y and tg_pId
        [unroll] for (tg_kId = 0; tg_kId < BLOCK_SIZE; ++tg_kId)
            [unroll] for (tg_pId = 0; tg_pId < BLOCK_SIZE; ++tg_pId)
            {
                uint writeChannelId = tg_KernelBaseId + groupThreadID.x * BLOCK_SIZE + tg_kId;
                uint writePixelId = tg_OutputPixelBaseId + groupThreadID.y * BLOCK_SIZE + tg_pId;
                float writeValue = dstA[tg_pId*BLOCK_SIZE+tg_kId];
                #if CHANNELS_FIRST
                    uint writeIndex = O.width * O.height * writeChannelId + writePixelId;
                #else
                    uint writeIndex = tg_NumKernels * writePixelId + writeChannelId;
                #endif
                #if LAX_KERNEL
                    bool canWriteChannel = (writeChannelId < tg_NumKernels);
                #else
                    bool canWriteChannel = true;
                #endif
                if ((writePixelId < tg_NumOuputPixels) && canWriteChannel)
                    O.FastSetWithActivation(writeIndex, writeValue);
            }
    #endif

    #undef X_OFFSET
    #undef W_OFFSET
    #undef LDS_
    #undef X_
    #undef W_
}
#undef CACHE_DEPTH
#undef CACHE_WIDTH
#undef SHUFFLE_FOR_COALESCED_LOAD
#undef SHUFFLE_FOR_COALESCED_STORE
#endif //KERNEL_PER_TG == 64

#if KERNEL_PER_TG == 16

#define CACHE_DEPTH 4                          // This kernel code supports only CACHE_DEPTH=4, this value can not be changed
#define PIXELS_PER_CACHE 256                   // This kernel code supports only PIXELS_PER_CACHE=256, this value can not be changed
#define NUMTHREADS_PER_TG 64                   // This kernel code supports only NUMTHREADS_PER_TG=64, this value can not be changed
#define PIXELS_READ_PER_THREAD_PER_CACHE       PIXELS_PER_CACHE/NUMTHREADS_PER_TG

#if CHANNELS_FIRST
    //NCHW
    #define PIXELS_CACHE_PAD 1
    #define SHUFFLE_FOR_COALESCED_LOAD 0       // Not needed for NCHW
    #define SHUFFLE_FOR_COALESCED_STORE 1
#else
    //NHWC
    #define PIXELS_CACHE_PAD 0                 // TODO not implemented for NHWC
    #define SHUFFLE_FOR_COALESCED_LOAD 1
    #define SHUFFLE_FOR_COALESCED_STORE 0      // Not implemented for NHWC, TODO (probably limited gain because of CACHE_DEPTH of 4)
#endif

#define PIXELS_PER_CACHE_AND_PAD ((PIXELS_PER_CACHE/BLOCK_SIZE)*(BLOCK_SIZE+PIXELS_CACHE_PAD))

#if SHUFFLE_FOR_COALESCED_STORE
    //A TG output [256pixels,16channels] = 4096 values, We will write two time 2048 values to DDR (8k LDS).
    groupshared float CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, LDS)[2048];
#else
    groupshared float CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, LDS)[(KERNEL_PER_TG+PIXELS_PER_CACHE_AND_PAD)*CACHE_DEPTH];
#endif
[numthreads(2,32,1)]
void FUNC_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE)(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint threadIndex : SV_GroupIndex)
{
    //This kernel assume the following:
    //Input:
    //  BatchCount == 1 (and thus layout is in fact CHW or HWC)
    //  C % CACHE_DEPTH==0 <-- only if STRICT_CHANNELS==1
    //Kernel:
    //  K%16==0 <-- only if LAX_KERNEL=0
    DISPATCH_ARGS(K.kernelCount, O.width * O.height, 1);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);
    #define LDS_ CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, LDS)
    #define X_OFFSET 0
    #define W_OFFSET CACHE_DEPTH*PIXELS_PER_CACHE_AND_PAD

    //Per thread group (scalar registers)
    uint tg_NumChannels = X.channels;
    uint tg_WidthX  = X.width;
    uint tg_HeightX = X.height;
    uint tg_WidthO  = O.width;
    uint tg_HeightO = O.height;
    uint tg_NumKernels = K.channels;
    uint tg_NumInputPixels = tg_WidthX*tg_HeightX;
    uint tg_NumOuputPixels = tg_WidthO*tg_HeightO;
    uint tg_KernelSpatialStride = tg_NumKernels*tg_NumChannels;
    uint tg_KernelBaseId = groupID.x * KERNEL_PER_TG;
    uint tg_OutputPixelBaseId = groupID.y * PIXELS_PER_CACHE;
    uint tg_kernelSpatialOffset = 0;

    //8x8 block, 8 kernels by 8 pixels
    //**********************************
    //* Kernel Ids  *  0  1  2  3  ...
    //**********************************
    //              *  ThreadIds
    // Pixel Ids  0 *  0  1  2  3 ...
    //            1 *  8  9 10 11 ...
    //            2 * 16 17 18 19 ...
    //            3 * 32 33 34 35 ...
    //            ... ...
    float dstA[BLOCK_SIZE*BLOCK_SIZE];

    //Load Bias [K] int dstA [Kernels, Pixels]
    uint tg_kId;
    uint tg_pId;
    [unroll] for (tg_pId = 0; tg_pId < BLOCK_SIZE; ++tg_pId)
        [unroll] for (tg_kId = 0; tg_kId < BLOCK_SIZE; ++tg_kId)
            dstA[tg_pId*BLOCK_SIZE+tg_kId] = B.FastGet(tg_KernelBaseId + groupThreadID.x * BLOCK_SIZE + tg_kId);

    //Loop spatialy on kernels
    for (uint tg_Dy = 0; tg_Dy < K.GetKernelHeight(); tg_Dy++)
    {
        for (uint tg_Dx = 0; tg_Dx < K.GetKernelWidth(); tg_Dx++)
        {
            for (uint tg_ChannelOffset = 0; tg_ChannelOffset < tg_NumChannels; tg_ChannelOffset += CACHE_DEPTH)
            {
                //Load from DDR to LDS: (16*CACHE_DEPTH=64 weights + 256*CACHE_DEPTH=1024 pixels) => 4352Bytes * CACHE_DEPTH.

                //K stored as HWCK, threadgroup is loading 64 kernels at a time to LDS in a linear fashion (4x16 kernels).
                //HW from tg_kernelSpatialOffset
                //C from tg_ChannelOffset (for TG) + threadIndex ([0-63]->[0-3])
                //K from tg_KernelBaseId (for TG) + threadIndex ([0-63])
                uint kernelCacheLoadOffset = threadIndex / 16;
                uint kernelLoadOffset = threadIndex % 16;
                uint kernelReadOffset = tg_kernelSpatialOffset + tg_NumKernels*(tg_ChannelOffset+kernelCacheLoadOffset) + tg_KernelBaseId + kernelLoadOffset;
                #if !STRICT_CHANNELS || LAX_KERNEL
                    kernelReadOffset = min(kernelReadOffset, K.GetLength()-1);
                #endif
                float tempW = K.FastGet(kernelReadOffset);

                #if SHUFFLE_FOR_COALESCED_LOAD
                    //Good for HWC
                    //TG is loading 256Pixels * CACHE_DEPTH to LDS in an attempt of linear fashion (16 pixels read per thread).
                    //would be better if CACHE_DEPTH would be bigger than 4 but LDS is the limiting factor here.
                    uint tg_PixelLoadIdx;
                    uint cacheLoadIdx = threadIndex % 4;
                    uint pixelLoadOffset = threadIndex / 4;
                    float tempX[CACHE_DEPTH*PIXELS_READ_PER_THREAD_PER_CACHE];//{channels*pixels}
                    [unroll] for (tg_PixelLoadIdx = 0; tg_PixelLoadIdx < PIXELS_READ_PER_THREAD_PER_CACHE*CACHE_DEPTH; ++tg_PixelLoadIdx)
                    {
                        //Compute input position and mask.
                        int outputPixelBaseId = tg_OutputPixelBaseId + PIXELS_READ_PER_THREAD_PER_CACHE*CACHE_DEPTH * tg_PixelLoadIdx + pixelLoadOffset;
                        int2 outputPixelCoords = int2(outputPixelBaseId % tg_WidthO, outputPixelBaseId / tg_WidthO);
                        int2 inputPixelCoords = outputPixelCoords * _Stride.xy - _Pad.xy + int2(tg_Dx, tg_Dy);
                        bool inputPixelMask = all( (inputPixelCoords >= 0) && (inputPixelCoords < float2(tg_WidthX, tg_HeightX)) );

                        int inputPixelId = inputPixelCoords.y * tg_WidthX + inputPixelCoords.x;
                        uint tg_InputChannelId = tg_ChannelOffset + cacheLoadIdx;
                        bool inputChannelMask = tg_InputChannelId < tg_NumChannels;
                        #if STRICT_CHANNELS
                            inputChannelMask = true;
                        #endif
                        #if CHANNELS_FIRST
                            uint pixelReadOffset = tg_NumInputPixels * tg_InputChannelId + inputPixelId;
                        #else
                            uint pixelReadOffset = tg_NumChannels * inputPixelId + tg_InputChannelId;
                        #endif
                        tempX[tg_PixelLoadIdx] = X.MaskedGet(inputPixelMask && inputChannelMask, pixelReadOffset);
                    }

                    [unroll] for (tg_PixelLoadIdx = 0; tg_PixelLoadIdx < PIXELS_READ_PER_THREAD_PER_CACHE*CACHE_DEPTH; ++tg_PixelLoadIdx)
                    {
                        LDS_[ X_OFFSET + cacheLoadIdx*PIXELS_PER_CACHE_AND_PAD + tg_PixelLoadIdx*PIXELS_READ_PER_THREAD_PER_CACHE*CACHE_DEPTH + pixelLoadOffset] = tempX[tg_PixelLoadIdx];
                    }
                #else
                    //Good for CHW
                    //TG is loading 256Pixels * CACHE_DEPTH to LDS in a linear fashion (4 channels * 4 pixels read per thread).
                    //Explicit register declaration as [unroll] won't unroll properly otherwise and introduce sync points.
                    uint tg_CacheLoadIdx;
                    uint tg_PixelLoadIdx;
                    float tempX[CACHE_DEPTH][PIXELS_READ_PER_THREAD_PER_CACHE];//{channels,pixels}
                    [unroll] for (tg_CacheLoadIdx = 0; tg_CacheLoadIdx < CACHE_DEPTH; ++tg_CacheLoadIdx)
                    {
                        [unroll] for (tg_PixelLoadIdx = 0; tg_PixelLoadIdx < PIXELS_READ_PER_THREAD_PER_CACHE; ++tg_PixelLoadIdx)
                        {
                            //Compute input position and mask.
                            int outputPixelBaseId = tg_OutputPixelBaseId + NUMTHREADS_PER_TG * tg_PixelLoadIdx + threadIndex;
                            int2 outputPixelCoords = int2(outputPixelBaseId % tg_WidthO, outputPixelBaseId / tg_WidthO);
                            int2 inputPixelCoords = outputPixelCoords * _Stride.xy - _Pad.xy + int2(tg_Dx, tg_Dy);
                            bool inputPixelMask = all( (inputPixelCoords >= 0) && (inputPixelCoords < float2(tg_WidthX, tg_HeightX)) );

                            int inputPixelId = inputPixelCoords.y * tg_WidthX + inputPixelCoords.x;
                            uint tg_InputChannelId = tg_ChannelOffset + tg_CacheLoadIdx;
                            bool inputChannelMask = tg_InputChannelId < tg_NumChannels;
                            #if STRICT_CHANNELS
                                inputChannelMask = true;
                            #endif
                            #if CHANNELS_FIRST
                                uint pixelReadOffset = tg_NumInputPixels * tg_InputChannelId + inputPixelId;
                            #else
                                uint pixelReadOffset = tg_NumChannels * inputPixelId + tg_InputChannelId;
                            #endif
                            tempX[tg_CacheLoadIdx][tg_PixelLoadIdx] = X.MaskedGet(inputPixelMask && inputChannelMask, pixelReadOffset);
                        }
                    }

                    [unroll] for (tg_CacheLoadIdx = 0; tg_CacheLoadIdx < CACHE_DEPTH; ++tg_CacheLoadIdx)
                    {
                        [unroll] for (tg_PixelLoadIdx = 0; tg_PixelLoadIdx < PIXELS_READ_PER_THREAD_PER_CACHE; ++tg_PixelLoadIdx)
                        {
                            uint ldsPixelCacheWriteIndex = tg_PixelLoadIdx*NUMTHREADS_PER_TG + threadIndex;
                            ldsPixelCacheWriteIndex += (ldsPixelCacheWriteIndex/BLOCK_SIZE) * PIXELS_CACHE_PAD;
                            LDS_[ X_OFFSET + tg_CacheLoadIdx*PIXELS_PER_CACHE_AND_PAD + ldsPixelCacheWriteIndex] = tempX[tg_CacheLoadIdx][tg_PixelLoadIdx];
                        }
                    }
                #endif
                LDS_[ W_OFFSET + kernelCacheLoadOffset*KERNEL_PER_TG + kernelLoadOffset ] = tempW;

                GroupMemoryBarrierWithGroupSync();

                //Inner loop
                uint ptrX = groupThreadID.y*(BLOCK_SIZE+PIXELS_CACHE_PAD) + X_OFFSET;
                uint ptrW = groupThreadID.x*BLOCK_SIZE + W_OFFSET;
                for (uint tg_CacheExecuteIdx = 0; tg_CacheExecuteIdx < CACHE_DEPTH; ++tg_CacheExecuteIdx)
                {
                    //Load LDS -> registers
                    float colOfX[BLOCK_SIZE];
                    float rowOfW[BLOCK_SIZE];
                    uint tg_q;
                    [unroll] for (tg_q = 0; tg_q < BLOCK_SIZE; ++tg_q)
                        colOfX[tg_q] = LDS_[ptrX + tg_q];
                    [unroll] for (tg_q = 0; tg_q < BLOCK_SIZE; ++tg_q)
                        rowOfW[tg_q] = LDS_[ptrW + tg_q];

                    ptrX += PIXELS_PER_CACHE_AND_PAD;
                    ptrW += KERNEL_PER_TG;

                    //Mads 8 pixels by 8 kernels matmul style --> 64 mads
                    [unroll] for (uint tg_X = 0; tg_X < BLOCK_SIZE; ++tg_X)
                        [unroll] for (uint tg_W = 0; tg_W < BLOCK_SIZE; ++tg_W)
                            dstA[tg_X*BLOCK_SIZE+tg_W] = ffma(colOfX[tg_X], rowOfW[tg_W], dstA[tg_X*BLOCK_SIZE+tg_W]);
                }

                GroupMemoryBarrierWithGroupSync();
            }

            tg_kernelSpatialOffset += tg_KernelSpatialStride;
        }
    }

    #if SHUFFLE_FOR_COALESCED_STORE && !LAX_KERNEL
        //-----------------------------------------------------
        //Use LDS to shuffle TG registers into coalesced writes
        //-----------------------------------------------------
        //A TG output [256pixels,16channels] = 4096 values, We will process [256,8] values at a time per TG.
        for (uint tg_registerChannelOffset = 0; tg_registerChannelOffset < BLOCK_SIZE; tg_registerChannelOffset += 4)
        {
            //Store 8 pixels x 4 channels per threads to LDS.
            uint ldsRowOffset = groupThreadID.x * 4;
            uint ldsPixelOffset = groupThreadID.y * BLOCK_SIZE;
            [unroll] for (tg_kId = 0; tg_kId < 4; ++tg_kId)
                [unroll] for (tg_pId = 0; tg_pId < BLOCK_SIZE; ++tg_pId)
                {
                    //To avoid bank conflict store in 32 groups [8pixelsGroups,4channelsGroups] each group contain 64 values [32pixels,2kernels] for a total of 2048 values [256pixels,8channels]
                    uint ldsOffsetOfGroup = NUMTHREADS_PER_TG * (tg_kId*BLOCK_SIZE+tg_pId);//64 * ([0,3]*8+[0,7]) = [0,1984]
                    LDS_[ldsOffsetOfGroup + threadIndex] = dstA[BLOCK_SIZE * tg_pId + (tg_registerChannelOffset + tg_kId)];
                }

            GroupMemoryBarrierWithGroupSync();

            //We have a buffers of [256pixels,8channels] floats, each thread will store [4pixels,8channels] so a threadgroup is storing 64 pixels at a time to DDR in a linear fashion.
            //Using a Macro as [unroll] on loop(groupID) won't unroll properly and thus introduce LDS/DDR sync points.
            #define WRITE_8CHANNELS_IF_POSSIBLE(groupID) \
            {\
                uint readPixelId = groupID * NUMTHREADS_PER_TG + threadIndex; \
                uint writePixelId = tg_OutputPixelBaseId + groupID * NUMTHREADS_PER_TG + threadIndex; \
                if (writePixelId < tg_NumOuputPixels) \
                { \
                    [unroll] for (tg_kId = 0; tg_kId < BLOCK_SIZE; ++tg_kId) \
                    { \
                        uint tg_kIdOfGroup = tg_kId % 4; \
                        uint pIdOfGroup = readPixelId % BLOCK_SIZE; \
                        uint ldsOffsetOfGroup = NUMTHREADS_PER_TG * (tg_kIdOfGroup * BLOCK_SIZE + pIdOfGroup); \
                        uint tg_kIdInGroup = (tg_kId - tg_kIdOfGroup) / 4; \
                        uint pIdInGroup = (readPixelId - pIdOfGroup) / BLOCK_SIZE; \
                        uint ldsOffsetInGroup = pIdInGroup * 2 + tg_kIdInGroup; \
                        uint readIndex = ldsOffsetOfGroup + ldsOffsetInGroup; \
                        uint writeChannelId = tg_KernelBaseId + tg_kId%4 + (tg_kId/4)*BLOCK_SIZE + tg_registerChannelOffset; \
                        uint writeIndex = O.width * O.height * writeChannelId + writePixelId; \
                        O.FastSetWithActivation(writeIndex, LDS_[readIndex]); \
                    } \
                } \
            }
            WRITE_8CHANNELS_IF_POSSIBLE(0)
            WRITE_8CHANNELS_IF_POSSIBLE(1)
            WRITE_8CHANNELS_IF_POSSIBLE(2)
            WRITE_8CHANNELS_IF_POSSIBLE(3)
            #undef WRITE_8CHANNELS_IF_POSSIBLE

            GroupMemoryBarrierWithGroupSync();
        }
    #else
        //-------------------------------
        //Directly store registers to DDR
        //-------------------------------
        //B does not require an offset as size == 1
        //C from tg_KernelBaseId, groupThreadID.x and tg_kId
        //HW from tg_OutputPixelBaseId, groupThreadID.y and tg_pId
        [unroll] for (tg_kId = 0; tg_kId < BLOCK_SIZE; ++tg_kId)
            [unroll] for (tg_pId = 0; tg_pId < BLOCK_SIZE; ++tg_pId)
            {
                uint writeChannelId = tg_KernelBaseId + groupThreadID.x * BLOCK_SIZE + tg_kId;
                uint writePixelId = tg_OutputPixelBaseId + groupThreadID.y * BLOCK_SIZE + tg_pId;
                float writeValue = dstA[tg_pId*BLOCK_SIZE+tg_kId];
                #if CHANNELS_FIRST
                    uint writeIndex = O.width * O.height * writeChannelId + writePixelId;
                #else
                    uint writeIndex = tg_NumKernels * writePixelId + writeChannelId;
                #endif
                #if LAX_KERNEL
                    bool canWriteChannel = (writeChannelId < tg_NumKernels);
                #else
                    bool canWriteChannel = true;
                #endif
                if ((writePixelId < tg_NumOuputPixels) && canWriteChannel)
                    O.FastSetWithActivation(writeIndex, writeValue);
            }
    #endif

    #undef X_OFFSET
    #undef W_OFFSET
    #undef LDS_
    #undef X_
    #undef W_
}
#undef CACHE_DEPTH
#undef PIXELS_READ_PER_THREAD_PER_CACHE
#undef PIXELS_PER_CACHE
#undef NUMTHREADS_PER_TG
#undef SHUFFLE_FOR_COALESCED_LOAD
#undef SHUFFLE_FOR_COALESCED_STORE
#endif //KERNEL_PER_TG == 16

#endif //BLOCK_SIZE == 8

#if BLOCK_SIZE == 4
#define BUF_OFFSET 0
#define CACHE_DEPTH 16 // This kernel code supports only CACHE_DEPTH=16, this value can not be changed
groupshared float CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, X)[CACHE_DEPTH*16*BLOCK_SIZE+(1-CHANNELS_FIRST)*CACHE_DEPTH];
groupshared float CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, W)[CACHE_DEPTH*16*BLOCK_SIZE];
[numthreads(16,16,1)]
void FUNC_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE)(uint3 dispatchThreadID : SV_DispatchThreadID, uint3 groupThreadID : SV_GroupThreadID, uint threadIndex : SV_GroupIndex)
{
    DISPATCH_ARGS(K.kernelCount, O.width * O.height * O.batch, 1);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    // [W*H, Ky*Kx*In] * [Ky*Kx*In, Out] => [W*H, Out]

    #define X_ CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, X)
    #define W_ CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, W)

    int x = (int)dispatchThreadID.x * BLOCK_SIZE; // output_channels
    int y = (int)dispatchThreadID.y * BLOCK_SIZE; // batch*width*height
    int tx = (int)groupThreadID.x;
    int ty = (int)groupThreadID.y;
    int bx = ((int)dispatchThreadID.x - (int)groupThreadID.x) * BLOCK_SIZE;
    int by = ((int)dispatchThreadID.y - (int)groupThreadID.y) * BLOCK_SIZE;
    int ti = (int)threadIndex;
    uint w      = O.width;
    uint h      = O.height;
    int batches = X.batch;
    int channels = X.channels;
    int widthX  = X.width;
    int heightX = X.height;
    int strideX = X.channels;
    int strideK = K.channels;
    int strideO = O.channels;
    int offsetX = BUF_OFFSET;
    int offsetK = BUF_OFFSET;
    int offsetO = BUF_OFFSET;

    float4 dstA[4];
    dstA[0].x = B.FastGet(x+0); dstA[0].y = B.FastGet(x+1); dstA[0].z = B.FastGet(x+2); dstA[0].w = B.FastGet(x+3);
    dstA[1].x = B.FastGet(x+0); dstA[1].y = B.FastGet(x+1); dstA[1].z = B.FastGet(x+2); dstA[1].w = B.FastGet(x+3);
    dstA[2].x = B.FastGet(x+0); dstA[2].y = B.FastGet(x+1); dstA[2].z = B.FastGet(x+2); dstA[2].w = B.FastGet(x+3);
    dstA[3].x = B.FastGet(x+0); dstA[3].y = B.FastGet(x+1); dstA[3].z = B.FastGet(x+2); dstA[3].w = B.FastGet(x+3);

    int readK = strideK * (ti>>6) + bx + (ti&63) + offsetK;
    #if STRICT_CHANNELS
    #else
    bool maskK = (bx + (ti&63)) < strideK;
    #endif

#if CHANNELS_FIRST
    uint centroidId = by + (ti&63);
    #if KERNEL_1x1
    int readX = strideX * (ti>>6) + centroidId;
    bool mask = readX < (int)X.GetLength();
    #else
    int batch = centroidId / w / h;
    int topY = (centroidId / w % h) * _Stride.y - _Pad.y;
    int leftX = (centroidId % w) * _Stride.x - _Pad.x;
    int cornerId = batch * heightX * widthX + topY * widthX + leftX;
    int readX = strideX * (ti>>6) + cornerId;
    bool mask;
    #endif
#else
    uint4 centroidId = uint4(
        (by + (ti>>4) +  0),
        (by + (ti>>4) + 16),
        (by + (ti>>4) + 32),
        (by + (ti>>4) + 48));
    #if KERNEL_1x1
    int4 readX = strideX * centroidId + (ti&15);
    bool4 mask = readX < (int)X.GetLength();
    #else
    int4 batch = centroidId / w / h;
    int4 topY = (centroidId / w % h) * _Stride.y - _Pad.y;
    int4 leftX = (centroidId % w) * _Stride.x - _Pad.x;
    int4 cornerId = batch * heightX * widthX + topY * widthX + leftX;
    int4 readX = strideX * cornerId + (ti&15);
    bool4 mask;
    #endif
#endif

#if KERNEL_1x1
    {
        {
#else
    for (int dy = 0; dy < (int)K.GetKernelHeight(); dy++)
    {
        for (int dx = 0; dx < (int)K.GetKernelWidth(); dx++)
        {
            int kernelOffsetX = (dy * widthX + dx) * strideX;
            mask =
                batch < batches &&
                topY + dy >= 0 &&
                topY + dy < heightX &&
                leftX + dx >= 0 &&
                leftX + dx < widthX;

            // 256 threads (256=numthreads(16,16,1)=16*16*1) are communally loading
            // blocks of 64pixels x 16channels from the global memory
            //
            // One block is read from X and one from K tensor
            // 4 reads with 256 threads (4=64*16/256) are necessary for each block

#endif // KERNEL_1x1
            for (int i = 0; i < channels; i += CACHE_DEPTH)
            {
                #if STRICT_CHANNELS
                #else
                if (i + CACHE_DEPTH > channels)
                {
                    int channelRemainder = channels - i;
                    [unroll] for (int j = 0; j < 4; ++j)
                    {
                        bool maskChannelsK = ti < 64 * (channelRemainder - j * 4);
                        bool maskChannelsX =
                            #if CHANNELS_FIRST
                            maskChannelsK;
                            #else
                            (ti&15) < channelRemainder;
                            #endif

                        W_[((ti>>6)<<6) + ((ti&3)<<4) + ((ti&63)>>2) + 256*j] = K.MaskedGet(maskK & maskChannelsK, readK);
                        readK += strideK * max(0, min(channelRemainder - j * 4, 4));

                        #if CHANNELS_FIRST
                        X_[ti + 256*j] =
                            #if KERNEL_1x1
                            X.MaskedGet(mask && maskChannelsX, readX + strideX * (i + j * 4) + offsetX);
                            #else
                            X.MaskedGet(mask && maskChannelsX, readX + strideX * (i + j * 4) + kernelOffsetX + offsetX);
                            #endif
                        #else
                        X_[(ti>>4) + 65*(ti&15) + 16*j] =
                            #if KERNEL_1x1
                            X.MaskedGet(mask[j] && maskChannelsX, readX[j] + i + offsetX);
                            #else
                            X.MaskedGet(mask[j] && maskChannelsX, readX[j] + i + kernelOffsetX + offsetX);
                            #endif
                        #endif
                    }
                }
                else
                #endif
                [unroll] for (int j = 0; j < 4; ++j)
                {
                    W_[((ti>>6)<<6) + ((ti&3)<<4) + ((ti&63)>>2) + 256*j] =
                        #if STRICT_CHANNELS
                        K.data[readK];
                        #else
                        K.MaskedGet(maskK, readK);
                        #endif
                    readK += strideK * 4;

                    #if CHANNELS_FIRST
                    X_[ti + 256*j] =
                        #if KERNEL_1x1
                        X.MaskedGet(mask, readX + strideX * (i + j * 4) + offsetX);
                        #else
                        X.MaskedGet(mask, readX + strideX * (i + j * 4) + kernelOffsetX + offsetX);
                        #endif
                    #else
                    X_[(ti>>4) + 65*(ti&15) + 16*j] =
                        #if KERNEL_1x1
                        X.MaskedGet(mask[j], readX[j] + i + offsetX);
                        #else
                        X.MaskedGet(mask[j], readX[j] + i + kernelOffsetX + offsetX);
                        #endif
                    #endif

                    #if DEBUG_CHECK_BOUNDS
                    if (
                        #if KERNEL_1x1
                        (readX[j] + i + offsetX < 0) ||
                        (readX[j] + i + offsetX >= (int)X.GetLength())
                        #else
                        (mask[j] && readX[j] + i + kernelOffsetX + offsetX < 0) ||
                        (mask[j] && readX[j] + i + kernelOffsetX + offsetX >= (int)X.GetLength())
                        #endif
                        )
                    {
                        // swamp X cache with dummy values when reading out of buffer
                        // this way we can detect out of buffer reads by comparing results from this kernel
                        // with the the reference implementation results
                        for (int q = 0; q < CACHE_DEPTH*16*BLOCK_SIZE+(1-CHANNELS_FIRST)*CACHE_DEPTH; ++q)
                            X_[q] = -1.0;
                    }
                    #endif
                }

                GroupMemoryBarrierWithGroupSync();

                int4 idX = int4(0,1,2,3);
                int4 idW = int4(0,16,32,48);
                int incX = 64 + (1-CHANNELS_FIRST);
                int incW = 64;

                for (int di = 0; di < CACHE_DEPTH; di++)
                {
                    float4 srcX = float4(
                        X_[idX.x + ty*4],
                        X_[idX.y + ty*4],
                        X_[idX.z + ty*4],
                        X_[idX.w + ty*4]);
                    float4 srcW = float4(
                        W_[idW.x + tx],
                        W_[idW.y + tx],
                        W_[idW.z + tx],
                        W_[idW.w + tx]
                    );
                    idX += incX;
                    idW += incW;

                    dstA[0].x = ffma(srcX.x, srcW.x, dstA[0].x);
                    dstA[0].y = ffma(srcX.x, srcW.y, dstA[0].y);
                    dstA[0].z = ffma(srcX.x, srcW.z, dstA[0].z);
                    dstA[0].w = ffma(srcX.x, srcW.w, dstA[0].w);

                    dstA[1].x = ffma(srcX.y, srcW.x, dstA[1].x);
                    dstA[1].y = ffma(srcX.y, srcW.y, dstA[1].y);
                    dstA[1].z = ffma(srcX.y, srcW.z, dstA[1].z);
                    dstA[1].w = ffma(srcX.y, srcW.w, dstA[1].w);

                    dstA[2].x = ffma(srcX.z, srcW.x, dstA[2].x);
                    dstA[2].y = ffma(srcX.z, srcW.y, dstA[2].y);
                    dstA[2].z = ffma(srcX.z, srcW.z, dstA[2].z);
                    dstA[2].w = ffma(srcX.z, srcW.w, dstA[2].w);

                    dstA[3].x = ffma(srcX.w, srcW.x, dstA[3].x);
                    dstA[3].y = ffma(srcX.w, srcW.y, dstA[3].y);
                    dstA[3].z = ffma(srcX.w, srcW.z, dstA[3].z);
                    dstA[3].w = ffma(srcX.w, srcW.w, dstA[3].w);
                }

                GroupMemoryBarrierWithGroupSync();
            }
        }
    }

    [unroll] for (int sy = 0; sy < 4 && y+sy < (int)w * (int)h * (int)O.batch; ++sy)
        [unroll] for (int sx = 0; sx < 4 && x+sx < strideO; ++sx)
            O.FastSetWithActivation(strideO * (y+sy) + x+sx + offsetO, dstA[sy][sx]);

    #undef X_
    #undef W_
}
#undef CACHE_DEPTH
#undef BUF_OFFSET
#endif
#undef KERNEL_NAME

NUMTHREADS((16,4,4), (8,4,4), (4,4,4))
void KERNEL_FUNC(Conv2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    uint2 leftCorner = _Pad.xy;
    uint2 rightCorner = uint2(X.width, X.height) + _Pad.xy;
    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = B.FastGet(k);
        for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
        {
            for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
            {
                uint2 pos = uint2(x, y) * _Stride.xy + uint2(dx, dy);

                for (uint c = 0; c < X.channels; ++c)
                {
                    float v = 0;

                    // WARNING: Mali-G71 performance drops 4x if this branching includes storing accumulator
                    if (!any(pos < leftCorner) && !any(pos >= rightCorner))
                        v = X.Get(n, pos.y - leftCorner.y, pos.x - leftCorner.x, c);
                    //acc = fastfma(v,  K.Get(dy, dx, c, k), acc);
                    acc += v * K.Get(dy, dx, c, k);
                }
            }
        }

        O.SetWithActivation(n, y, x, k, acc);
    }
}


#define SIZE_W 4
#define SIZE_H 2
NUMTHREADS((64, 2, 2), (32, 2, 2), (16, 2, 2))
void KERNEL_FUNC(Conv2D_RegisterBlock4x2)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x*SIZE_W >= O.width) return;
    if (y*SIZE_H >= O.height) return;

    uint2 leftCorner = _Pad.xy;
    uint2 rightCorner = uint2(X.width, X.height) + _Pad.xy;
    for (uint n = 0; n < O.batch; ++n)
    {
        float acc[SIZE_H*SIZE_W];
        uint q;
        [unroll]
        for (q = 0; q < SIZE_H*SIZE_W; ++q)
            acc[q] = B.FastGet(k);
        for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
        {
            for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
            {
                uint2 pos[SIZE_H*SIZE_W];
                [unroll]
                for (q = 0; q < SIZE_H*SIZE_W; ++q)
                    pos[q] = uint2(x*SIZE_W+(q%SIZE_W), y*SIZE_H+(q/SIZE_W)) * _Stride.xy + uint2(dx, dy);

                for (uint c = 0; c < X.channels; ++c)
                    [unroll]
                    for (q = 0; q < SIZE_H*SIZE_W; ++q)
                        if (all(pos[q] >= leftCorner) && all(pos[q] < rightCorner))
                            acc[q] = fastfma(X.Get(n, pos[q] - leftCorner, c), K.Get(dy, dx, c, k), acc[q]);
            }
        }

        [unroll]
        for (q = 0; q < SIZE_H*SIZE_W; ++q)
            O.SetWithActivation(n, y*SIZE_H+(q/SIZE_W), x*SIZE_W+(q%SIZE_W), k, acc[q]);
    }
}
#undef SIZE_W
#undef SIZE_H

#define CONV2D_L1CACHED(L1CACHESIZE, SIZE, FMA) \
groupshared float Conv2D_L1Cached##L1CACHESIZE##_Reg_Loop_safe_X[SIZE*SIZE][L1CACHESIZE];\
[numthreads(L1CACHESIZE, 1, 1)]\
void KERNEL_FUNC(Conv2D_L1Cached##L1CACHESIZE##_RegisterBlock##SIZE##x##SIZE)(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID)\
{\
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);\
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);\
\
    uint k = L1CACHESIZE * groupID.x + groupThreadID.x;\
    uint x = groupID.y;\
    uint y = groupID.z;\
\
    if (x*SIZE >= O.width) return;\
    if (y*SIZE >= O.height) return;\
\
    for (uint n = 0; n < O.batch; ++n)\
    {\
        float acc[SIZE*SIZE];\
        uint q;\
        [unroll]\
        for (q = 0; q < SIZE*SIZE; ++q)\
            acc[q] = B.SafeGet(k);\
\
        for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)\
        {\
            for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)\
            {\
                uint2 pos[SIZE*SIZE];\
                [unroll]\
                for (q = 0; q < SIZE*SIZE; ++q)\
                    pos[q] = uint2(x*SIZE+(q%SIZE), y*SIZE+(q/SIZE)) * _Stride.xy + uint2(dx, dy);\
\
                for (uint c = 0; c < X.channels; c += L1CACHESIZE)\
                {\
                    uint dc = groupThreadID.x;\
                    [unroll]\
                    for (q = 0; q < SIZE*SIZE; ++q)\
                        Conv2D_L1Cached##L1CACHESIZE##_Reg_Loop_safe_X[q][dc] = X.SafeGet(n, pos[q], c + dc, _Pad.xy);\
                    GroupMemoryBarrierWithGroupSync();\
\
                    if (k < K.channels)\
                    {\
                        uint kIndex = K.IndexHWC(dy, dx, c, k);\
                        for (dc = 0; dc < L1CACHESIZE; ++dc)\
                        {\
                            [unroll]\
                            for (q = 0; q < SIZE*SIZE; ++q)\
                                acc[q] = FMA(Conv2D_L1Cached##L1CACHESIZE##_Reg_Loop_safe_X[q][dc], K.data[kIndex], acc[q]);\
                            kIndex += K.channels;\
                        }\
                    }\
                    GroupMemoryBarrierWithGroupSync();\
                }\
            }\
        }\
\
        uint remainderW = (O.width - x*SIZE);\
        uint remainderH = (O.height - y*SIZE);\
\
        if (k < K.channels)\
            [unroll]\
            for (q = 0; q < SIZE*SIZE; ++q)\
                if (q/SIZE < remainderH && q%SIZE < remainderW)\
                    O.SetWithActivation(n, y*SIZE+(q/SIZE), x*SIZE+(q%SIZE), k, acc[q]);\
    }\
\
}

CONV2D_L1CACHED(64,4, fastfma)
CONV2D_L1CACHED(32,4, fastfma)


// IDEA: iterate over channels in the inner loop - needs channels first layout
NUMTHREADS((16,4,4), (8,4,4), (4,4,4))
void KERNEL_FUNC(DepthwiseConv2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    uint2 leftCorner = _Pad.xy;
    uint2 rightCorner = uint2(X.width, X.height) + _Pad.xy;

    uint2 leftKernelCorner = uint2(x, y) * _Stride.xy;
    uint2 rightKernelCorner = leftKernelCorner + uint2(K.GetKernelWidth(), K.GetKernelHeight());

    if (any(leftKernelCorner < leftCorner) || any(rightKernelCorner >= rightCorner))
    {
        // path with edge-cases checks
        for (uint n = 0; n < O.batch; ++n)
        {
            float acc = B.FastGet(k);
            for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
                for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
                {
                    uint2 pos = leftKernelCorner + uint2(dx, dy);
                    if (any(pos < leftCorner)) continue;
                    if (any(pos >= rightCorner)) continue;

                    acc = fastfma(
                        X.Get(n, pos.y - leftCorner.y, pos.x - leftCorner.x, k),
                        K.Get(dy, dx, 0, k),
                        acc);
                }

            O.SetWithActivation(n, y, x, k, acc);
        }
    }
    else
    {
        // kernel is guaranteed to be within X,
        // no need to check against edge-cases
        leftKernelCorner -= leftCorner;
        for (uint n = 0; n < O.batch; ++n)
        {
            float acc = B.FastGet(k);
            for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
                for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
                {
                    uint2 pos = leftKernelCorner + uint2(dx, dy);

                    acc = fastfma(
                        X.Get(n, pos, k),
                        K.Get(dy, dx, 0, k),
                        acc);
                }

            O.SetWithActivation(n, y, x, k, acc);
        }
    }
}


NUMTHREADS((16, 4, 4), (8, 4, 4), (4, 4, 4))
void Conv2DTransFlipKernel(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    TENSOR_SHARED_MODEL(K, WBK); TENSOR_SHARED_MODEL(B, WBK); TENSOR_ARG_RW(O)

    uint k = dispatchThreadID.x;
    uint c = dispatchThreadID.y;
    uint z = dispatchThreadID.z; // x + KWidth * y

    uint x = z % K.GetKernelWidth();
    uint y = z / K.GetKernelWidth();

    if (c >= K.GetKernelDepth()) return;
    if (k >= K.GetKernelCount()) return;
    if (z >= K.GetKernelHeight() * K.GetKernelWidth()) return;

    float v = K.Get(K.GetKernelHeight() - 1 - y, K.GetKernelWidth() - 1 - x, c, k);
    O.Set(y, x, c, k, v);
    O.FastSet(K.GetLength() + k, B.FastGet(k));
}

NUMTHREADS((16, 4, 4), (8, 4, 4), (4, 4, 4))
void KERNEL_FUNC(Conv2DTransPadFill)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= X.channels) return;
    if (x >= X.width) return;
    if (y >= X.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float v = X.Get(n, y, x, c);

        uint oy = y * _Stride.y;
        uint ox = x * _Stride.x;
        O.Set(n, oy, ox, c, v);

        // 0-pad accordingly:
        // stride number of 0 between values of X
        // outputAdjustment number of 0 at the end of X
        const uint2 padding = y < (X.height - 1) && x < (X.width - 1) ? _Stride.xy : _Pad.xy;
        for (uint dy = 0; dy < padding.y; ++dy)
            for (uint dx = 0; dx < padding.x; ++dx)
            {
                if (dy != 0 && dx != 0)
                    O.Set(n, oy + dy, ox + dx, c, 0.0);
            }
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Conv2DTrans)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    uint2 strideMask = _Stride.xy - 1;

    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = B.FastGet(k);
        for (uint dy = (y + _Pad.y) & strideMask.y; dy < K.GetKernelHeight(); dy += _Stride.y)
        {
            for (uint dx = (x + _Pad.x) & strideMask.x; dx < K.GetKernelWidth(); dx += _Stride.x)
            {

                uint2 pos = uint2(x + dx, y + dy);
                uint2 opos = (pos - _Pad.xy) / _Stride.xy;

                if (any(opos >= uint2(X.width, X.height))) continue;
                if (any(pos < _Pad.xy)) continue;

                for (uint c = 0; c < X.channels; ++c)
                {
                    acc = fastfma(  X.Get(n, opos.y, opos.x, c),
                                    K.Get(  K.GetKernelHeight() - 1 - dy,
                                            K.GetKernelWidth()  - 1 - dx, c, k),
                                    acc);
                }
            }
        }

        O.SetWithActivation(n, y, x, k, acc);
    }
}

#if defined(MAX_KERNEL_SIZE) && defined(GROUP_SIZE_X) && defined(GROUP_SIZE_Y)

#if CHANNELS_FIRST
    #define CONV2DTRANS_NAME(KERNEL,TGX,TGY) Conv2DTrans_KernelCached_K##KERNEL##x##KERNEL##_T##TGX##x##TGY##_NCHW
#else
    #define CONV2DTRANS_NAME(KERNEL,TGX,TGY) Conv2DTrans_KernelCached_K##KERNEL##x##KERNEL##_T##TGX##x##TGY##_NHWC
#endif
groupshared float Conv2DTrans_SharedKernel[MAX_KERNEL_SIZE][MAX_KERNEL_SIZE][GROUP_SIZE_X*GROUP_SIZE_Y];
groupshared float Conv2DTrans_SharedBias;
[numthreads(1,GROUP_SIZE_X,GROUP_SIZE_Y)]
void CONV2DTRANS_NAME(MAX_KERNEL_SIZE, GROUP_SIZE_X,GROUP_SIZE_Y)(uint3 dispatchThreadID : SV_DispatchThreadID, uint groupIndex: SV_GroupIndex)
{
    //Constraints:
    // C <= GROUP_SIZE_X*GROUP_SIZE_Y
    // K <= MAX_KERNEL_SIZExMAX_KERNEL_SIZE
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    //Dispatch organisation:
    //  a thread  = write to [:,y,x,k] ie all batch but a single 2d pos and feature.
    //  a thread group = handle 1 feature in a GROUP_SIZExGROUP_SIZE x,y region, it loop other all batch, input channel count need to be <= GROUP_SIZE*GROUP_SIZE

    //LDS allocation
    //  we have 1 feature and up to GROUP_SIZE_X*GROUP_SIZE_Y channels per thread group, batch all use the same kernels,
    //  thus LDS is [MAX_KERNEL_SIZE][MAX_KERNEL_SIZE][GROUP_SIZE_X*GROUP_SIZE_Y]

    //Loading to LDS
    //  Each threads load a 2D kernel for a different channel into LDS
    for(uint dy = 0; dy < K.GetKernelWidth(); ++dy)
    {
        for(uint dx = 0; dx < K.GetKernelHeight(); ++dx)
        {
            uint channelToLoadIndex = groupIndex;
            if((channelToLoadIndex < X.channels) && (k < K.channels))
                Conv2DTrans_SharedKernel[dy][dx][channelToLoadIndex] = K.Get(K.GetKernelHeight() - 1 - dy,K.GetKernelWidth() - 1 - dx, channelToLoadIndex, k);
        }
    }
    //  first thread also load bias to LDS
    if (groupIndex == 0)
        Conv2DTrans_SharedBias = B.FastGet(k);

    //Wait for all load to complete
    GroupMemoryBarrierWithGroupSync();

    // Outside of target tensor, nothing to write to or compute exit.
    if (x >= O.width) return;
    if (y >= O.height) return;
    if (k >= K.channels) return;

    // Apply kernels from LDS to all batches and write result out (per batch as input differ)
    uint2 strideMask = _Stride.xy - 1;
    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = Conv2DTrans_SharedBias;
        for (uint dy = (y + _Pad.y) & strideMask.y; dy < K.GetKernelHeight(); dy += _Stride.y)
        {
            for (uint dx = (x + _Pad.x) & strideMask.x; dx < K.GetKernelWidth(); dx += _Stride.x)
            {
                uint2 pos = uint2(x + dx, y + dy);
                uint2 opos = (pos - _Pad.xy) / _Stride.xy;
                if (any(opos >= uint2(X.width, X.height))) continue;
                if (any(pos < _Pad.xy)) continue;

                for (uint c = 0; c < X.channels; ++c)
                {
                    acc = fastfma(X.Get(n, opos.y, opos.x, c),
                        Conv2DTrans_SharedKernel[dy][dx][c],
                        acc);
                }
            }
        }
        O.SetWithActivation(n, y, x, k, acc);
    }
}
#undef CONV2DTRANS_NAME
#endif //defined(MAX_KERNEL_SIZE) && defined(GROUP_SIZE_X) && defined(GROUP_SIZE_Y)




// https://github.com/andravin/wincnn
// https://arxiv.org/pdf/1509.09308.pdf
// Winograd: 4x4 image, 3x3 kernel, 2x2 output
static const float4x3 Winograd_G = float4x3(float3(1, 0, 0), float3(0.5, 0.5, 0.5), float3(0.5, -0.5, 0.5), float3(0, 0, 1));
static const float3x4 Winograd_GT = transpose(Winograd_G);
[numthreads(1, 1, 1)]
void KernelWinograd_3x3(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    TENSOR_SHARED_MODEL(K, WBK); TENSOR_SHARED_MODEL(B, WBK); TENSOR_ARG_RW(O)

    uint k = dispatchThreadID.x;
    uint c = dispatchThreadID.y;
    uint i = dispatchThreadID.z;

    if (c >= K.GetKernelDepth()) return;
    if (k >= K.GetKernelCount()) return;

    float3x3 g;
    g[0][0] = K.Get(0, 0, c, k);
    g[0][1] = K.Get(0, 1, c, k);
    g[0][2] = K.Get(0, 2, c, k);
    g[1][0] = K.Get(1, 0, c, k);
    g[1][1] = K.Get(1, 1, c, k);
    g[1][2] = K.Get(1, 2, c, k);
    g[2][0] = K.Get(2, 0, c, k);
    g[2][1] = K.Get(2, 1, c, k);
    g[2][2] = K.Get(2, 2, c, k);

    float4x4 v = mul(Winograd_G, mul(g, Winograd_GT));

    O.Set(0, 0, c, k, v[0][0]);
    O.Set(1, 0, c, k, v[1][0]);
    O.Set(2, 0, c, k, v[2][0]);
    O.Set(3, 0, c, k, v[3][0]);
    O.Set(0, 1, c, k, v[0][1]);
    O.Set(1, 1, c, k, v[1][1]);
    O.Set(2, 1, c, k, v[2][1]);
    O.Set(3, 1, c, k, v[3][1]);
    O.Set(0, 2, c, k, v[0][2]);
    O.Set(1, 2, c, k, v[1][2]);
    O.Set(2, 2, c, k, v[2][2]);
    O.Set(3, 2, c, k, v[3][2]);
    O.Set(0, 3, c, k, v[0][3]);
    O.Set(1, 3, c, k, v[1][3]);
    O.Set(2, 3, c, k, v[2][3]);
    O.Set(3, 3, c, k, v[3][3]);

    uint kLength = (K.GetKernelHeight() + 1) * (K.GetKernelWidth() + 1) * K.GetKernelDepth() * K.GetKernelCount();
    if (i < B.GetLength())
        O.FastSet(kLength + i, B.FastGet(i));
}

// BT x u x B, used mathematica to expresse the opperation using only +/-
float4x4 ApplyWinnogradB(float4x4 d)
{
    return float4x4(float4( d[0][0] - d[0][2] - d[2][0] + d[2][2],  d[0][1] + d[0][2] - d[2][1] - d[2][2], -d[0][1] + d[0][2] + d[2][1] - d[2][2], -d[0][1] + d[0][3] + d[2][1] - d[2][3]),
                    float4( d[1][0] - d[1][2] + d[2][0] - d[2][2],  d[1][1] + d[1][2] + d[2][1] + d[2][2], -d[1][1] + d[1][2] - d[2][1] + d[2][2], -d[1][1] + d[1][3] - d[2][1] + d[2][3]),
                    float4(-d[1][0] + d[1][2] + d[2][0] - d[2][2], -d[1][1] - d[1][2] + d[2][1] + d[2][2],  d[1][1] - d[1][2] - d[2][1] + d[2][2],  d[1][1] - d[1][3] - d[2][1] + d[2][3]),
                    float4(-d[1][0] + d[1][2] + d[3][0] - d[3][2], -d[1][1] - d[1][2] + d[3][1] + d[3][2],  d[1][1] - d[1][2] - d[3][1] + d[3][2],  d[1][1] - d[1][3] - d[3][1] + d[3][3])
        );

}
// A x u x A, used mathematica to expresse the opperation using only +/-
float2x2 ApplyWinnogradA(float4x4 uv)
{
    return float2x2(float2(uv[0][0] + uv[0][1] + uv[0][2] + uv[1][0] + uv[1][1] + uv[1][2] + uv[2][0] + uv[2][1] + uv[2][2], uv[0][1] - uv[0][2] + uv[0][3] + uv[1][1] - uv[1][2] + uv[1][3] + uv[2][1] - uv[2][2] + uv[2][3]),
                    float2(uv[1][0] + uv[1][1] + uv[1][2] - uv[2][0] - uv[2][1] - uv[2][2] + uv[3][0] + uv[3][1] + uv[3][2], uv[1][1] - uv[1][2] + uv[1][3] - uv[2][1] + uv[2][2] - uv[2][3] + uv[3][1] - uv[3][2] + uv[3][3])
        );
}

#undef GROUP_SIZE
#define GROUP_SIZE 2

// https://graphics.stanford.edu/~seander/bithacks.html#IntegerMinOrMax
#define min_no_branch(x,y) (y ^ ((x ^ y) & -(x < y)))

[numthreads(32, GROUP_SIZE, GROUP_SIZE)]
void KERNEL_FUNC(Conv2DWinograd_2x2_3x3)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    if (k >= K.channels) return;

    uint2 index = 2 * dispatchThreadID.yz;

    uint2 pad = uint2(_Pad[0], _Pad[1]);
    uint2 XDim = uint2(X.width, X.height);


    // empirically faster on android
    // 0 if index - pad < 0 || index - pad > dim, given that we are dealing with uints, index - pad < 0 <=> index - pad ~= INTMAX
    // assuming usual input dim it should be fine.
    float4x4 paddingMask;
    paddingMask[0][0] = all(index.xy + uint2(0, 0) - pad < XDim);
    paddingMask[0][1] = all(index.xy + uint2(1, 0) - pad < XDim);
    paddingMask[0][2] = all(index.xy + uint2(2, 0) - pad < XDim);
    paddingMask[0][3] = all(index.xy + uint2(3, 0) - pad < XDim);
    paddingMask[1][0] = all(index.xy + uint2(0, 1) - pad < XDim);
    paddingMask[1][1] = all(index.xy + uint2(1, 1) - pad < XDim);
    paddingMask[1][2] = all(index.xy + uint2(2, 1) - pad < XDim);
    paddingMask[1][3] = all(index.xy + uint2(3, 1) - pad < XDim);
    paddingMask[2][0] = all(index.xy + uint2(0, 2) - pad < XDim);
    paddingMask[2][1] = all(index.xy + uint2(1, 2) - pad < XDim);
    paddingMask[2][2] = all(index.xy + uint2(2, 2) - pad < XDim);
    paddingMask[2][3] = all(index.xy + uint2(3, 2) - pad < XDim);
    paddingMask[3][0] = all(index.xy + uint2(0, 3) - pad < XDim);
    paddingMask[3][1] = all(index.xy + uint2(1, 3) - pad < XDim);
    paddingMask[3][2] = all(index.xy + uint2(2, 3) - pad < XDim);
    paddingMask[3][3] = all(index.xy + uint2(3, 3) - pad < XDim);


    for (uint n = 0; n < O.batch; ++n)
    {
        float2x2 acc = B.FastGet(k);

        for (uint c = 0; c < X.channels; ++c)
        {
            // 16 loads per thread
            float4x4 d;
            d[0][0] = X.Get(n, min_no_branch(index.xy + uint2(0, 0) - pad, XDim-1), c);
            d[0][1] = X.Get(n, min_no_branch(index.xy + uint2(1, 0) - pad, XDim-1), c);
            d[0][2] = X.Get(n, min_no_branch(index.xy + uint2(2, 0) - pad, XDim-1), c);
            d[0][3] = X.Get(n, min_no_branch(index.xy + uint2(3, 0) - pad, XDim-1), c);
            d[1][0] = X.Get(n, min_no_branch(index.xy + uint2(0, 1) - pad, XDim-1), c);
            d[1][1] = X.Get(n, min_no_branch(index.xy + uint2(1, 1) - pad, XDim-1), c);
            d[1][2] = X.Get(n, min_no_branch(index.xy + uint2(2, 1) - pad, XDim-1), c);
            d[1][3] = X.Get(n, min_no_branch(index.xy + uint2(3, 1) - pad, XDim-1), c);
            d[2][0] = X.Get(n, min_no_branch(index.xy + uint2(0, 2) - pad, XDim-1), c);
            d[2][1] = X.Get(n, min_no_branch(index.xy + uint2(1, 2) - pad, XDim-1), c);
            d[2][2] = X.Get(n, min_no_branch(index.xy + uint2(2, 2) - pad, XDim-1), c);
            d[2][3] = X.Get(n, min_no_branch(index.xy + uint2(3, 2) - pad, XDim-1), c);
            d[3][0] = X.Get(n, min_no_branch(index.xy + uint2(0, 3) - pad, XDim-1), c);
            d[3][1] = X.Get(n, min_no_branch(index.xy + uint2(1, 3) - pad, XDim-1), c);
            d[3][2] = X.Get(n, min_no_branch(index.xy + uint2(2, 3) - pad, XDim-1), c);
            d[3][3] = X.Get(n, min_no_branch(index.xy + uint2(3, 3) - pad, XDim-1), c);

            d = d * paddingMask;

            float4x4 v;
            v[0][0] = K.Get(0, 0, c, k);
            v[0][1] = K.Get(0, 1, c, k);
            v[0][2] = K.Get(0, 2, c, k);
            v[0][3] = K.Get(0, 3, c, k);
            v[1][0] = K.Get(1, 0, c, k);
            v[1][1] = K.Get(1, 1, c, k);
            v[1][2] = K.Get(1, 2, c, k);
            v[1][3] = K.Get(1, 3, c, k);
            v[2][0] = K.Get(2, 0, c, k);
            v[2][1] = K.Get(2, 1, c, k);
            v[2][2] = K.Get(2, 2, c, k);
            v[2][3] = K.Get(2, 3, c, k);
            v[3][0] = K.Get(3, 0, c, k);
            v[3][1] = K.Get(3, 1, c, k);
            v[3][2] = K.Get(3, 2, c, k);
            v[3][3] = K.Get(3, 3, c, k);

            float4x4 u = ApplyWinnogradB(d);
            float2x2 y = ApplyWinnogradA(v*u);
            acc += y;
        }

        // 4 writes per thread
        if(index.y < O.height && index.x < O.width)
        O.SetWithActivation(n, index.y + 0, index.x + 0, k, acc[0][0]);
        if (index.y + 1 < O.height && index.x < O.width)
        O.SetWithActivation(n, index.y + 1, index.x + 0, k, acc[1][0]);
        if (index.y < O.height && index.x + 1 < O.width)
        O.SetWithActivation(n, index.y + 0, index.x + 1, k, acc[0][1]);
        if (index.y + 1 < O.height && index.x + 1 < O.width)
        O.SetWithActivation(n, index.y + 1, index.x + 1, k, acc[1][1]);
    }
}
