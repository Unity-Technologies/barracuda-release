#pragma kernel Dense_NHWC CHANNELS_FIRST=0
#pragma kernel Dense_NCHW CHANNELS_FIRST=1
#pragma kernel Conv2D_NHWC CHANNELS_FIRST=0
#pragma kernel Conv2D_NCHW CHANNELS_FIRST=1
#pragma kernel Conv2DWinograd_2x2_3x3_NHWC CHANNELS_FIRST=0
#pragma kernel Conv2DWinograd_2x2_3x3_NCHW CHANNELS_FIRST=1
#pragma kernel DepthwiseConv2D_NHWC CHANNELS_FIRST=0
#pragma kernel DepthwiseConv2D_NCHW CHANNELS_FIRST=1
#pragma kernel Conv2DTrans_NHWC CHANNELS_FIRST=0
#pragma kernel Conv2DTrans_NCHW CHANNELS_FIRST=1
#pragma kernel Upsample2D_NHWC CHANNELS_FIRST=0
#pragma kernel Upsample2D_NCHW CHANNELS_FIRST=1
#pragma kernel UpsampleBilinear2D_NHWC CHANNELS_FIRST=0
#pragma kernel UpsampleBilinear2D_NCHW CHANNELS_FIRST=1
#pragma kernel Unstride2D_NHWC CHANNELS_FIRST=0
#pragma kernel Unstride2D_NCHW CHANNELS_FIRST=1
#pragma kernel MaxPool2D_NHWC CHANNELS_FIRST=0
#pragma kernel MaxPool2D_NCHW CHANNELS_FIRST=1
#pragma kernel AvgPool2D_NHWC CHANNELS_FIRST=0
#pragma kernel AvgPool2D_NCHW CHANNELS_FIRST=1
#pragma kernel GlobalMaxPool2D_NHWC CHANNELS_FIRST=0
#pragma kernel GlobalMaxPool2D_NCHW CHANNELS_FIRST=1
#pragma kernel GlobalAvgPool2D_NHWC CHANNELS_FIRST=0
#pragma kernel GlobalAvgPool2D_NCHW CHANNELS_FIRST=1
#pragma kernel GlobalAvgVariancePool2D_NHWC CHANNELS_FIRST=0
#pragma kernel GlobalAvgVariancePool2D_NCHW CHANNELS_FIRST=1
#pragma kernel ScaleBias_NHWC CHANNELS_FIRST=0
#pragma kernel ScaleBias_NCHW CHANNELS_FIRST=1
#pragma kernel InstanceNorm_NHWC CHANNELS_FIRST=0
#pragma kernel InstanceNorm_NCHW CHANNELS_FIRST=1
#pragma kernel Dropout_NHWC CHANNELS_FIRST=0
#pragma kernel Dropout_NCHW CHANNELS_FIRST=1
#pragma kernel Relu_NHWC CHANNELS_FIRST=0
#pragma kernel Relu_NCHW CHANNELS_FIRST=1
#pragma kernel Neg_NHWC CHANNELS_FIRST=0
#pragma kernel Neg_NCHW CHANNELS_FIRST=1
#pragma kernel Reciprocal_NHWC CHANNELS_FIRST=0
#pragma kernel Reciprocal_NCHW CHANNELS_FIRST=1
#pragma kernel Swish_NHWC CHANNELS_FIRST=0
#pragma kernel Swish_NCHW CHANNELS_FIRST=1
#pragma kernel Softmax_NHWC CHANNELS_FIRST=0
#pragma kernel Softmax_NCHW CHANNELS_FIRST=1
#pragma kernel LogSoftmax_NHWC CHANNELS_FIRST=0
#pragma kernel LogSoftmax_NCHW CHANNELS_FIRST=1
#pragma kernel Tanh_NHWC CHANNELS_FIRST=0
#pragma kernel Tanh_NCHW CHANNELS_FIRST=1
#pragma kernel Sigmoid_NHWC CHANNELS_FIRST=0
#pragma kernel Sigmoid_NCHW CHANNELS_FIRST=1
#pragma kernel Relu6_NHWC CHANNELS_FIRST=0
#pragma kernel Relu6_NCHW CHANNELS_FIRST=1
#pragma kernel Elu_NHWC CHANNELS_FIRST=0
#pragma kernel Elu_NCHW CHANNELS_FIRST=1
#pragma kernel LeakyRelu_NHWC CHANNELS_FIRST=0
#pragma kernel LeakyRelu_NCHW CHANNELS_FIRST=1
#pragma kernel PRelu_NHWC CHANNELS_FIRST=0
#pragma kernel PRelu_NCHW CHANNELS_FIRST=1
#pragma kernel Selu_NHWC CHANNELS_FIRST=0
#pragma kernel Selu_NCHW CHANNELS_FIRST=1
#pragma kernel Exp_NHWC CHANNELS_FIRST=0
#pragma kernel Exp_NCHW CHANNELS_FIRST=1
#pragma kernel Log_NHWC CHANNELS_FIRST=0
#pragma kernel Log_NCHW CHANNELS_FIRST=1
#pragma kernel Sqrt_NHWC CHANNELS_FIRST=0
#pragma kernel Sqrt_NCHW CHANNELS_FIRST=1
#pragma kernel Pow_NHWC CHANNELS_FIRST=0
#pragma kernel Pow_NCHW CHANNELS_FIRST=1
#pragma kernel Clip_NHWC CHANNELS_FIRST=0
#pragma kernel Clip_NCHW CHANNELS_FIRST=1
#pragma kernel Copy_NHWC CHANNELS_FIRST=0
#pragma kernel Copy_NCHW CHANNELS_FIRST=1
#pragma kernel ReshapeFromNHWCModel_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastAdd_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastAdd_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastSub_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastSub_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastMul_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastMul_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastDiv_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastDiv_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastPow_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastPow_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastMin_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastMin_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastMax_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastMax_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastMean_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastMean_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastGreater_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastGreater_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastGreaterEqual_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastGreaterEqual_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastLess_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastLess_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastLessEqual_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastLessEqual_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastEqual_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastEqual_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastLogicalOr_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastLogicalOr_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastLogicalAnd_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastLogicalAnd_NCHW CHANNELS_FIRST=1
#pragma kernel BroadcastLogicalXor_NHWC CHANNELS_FIRST=0
#pragma kernel BroadcastLogicalXor_NCHW CHANNELS_FIRST=1
#pragma kernel LogicalNot_NHWC CHANNELS_FIRST=0
#pragma kernel LogicalNot_NCHW CHANNELS_FIRST=1
#pragma kernel ReduceMin_NHWC CHANNELS_FIRST=0
#pragma kernel ReduceMin_NCHW CHANNELS_FIRST=1
#pragma kernel ReduceMax_NHWC CHANNELS_FIRST=0
#pragma kernel ReduceMax_NCHW CHANNELS_FIRST=1
#pragma kernel ReduceSum_NHWC CHANNELS_FIRST=0
#pragma kernel ReduceSum_NCHW CHANNELS_FIRST=1
#pragma kernel ReduceMean_NHWC CHANNELS_FIRST=0
#pragma kernel ReduceMean_NCHW CHANNELS_FIRST=1
#pragma kernel ReduceProd_NHWC CHANNELS_FIRST=0
#pragma kernel ReduceProd_NCHW CHANNELS_FIRST=1
#pragma kernel TextureToTensor_NHWC CHANNELS_FIRST=0
#pragma kernel TextureToTensor_NCHW CHANNELS_FIRST=1
#pragma kernel TensorToTextureNoLUT_NHWC SUFFIX=NoLUT CHANNELS_FIRST=0
#pragma kernel TensorToTextureNoLUT_NCHW SUFFIX=NoLUT CHANNELS_FIRST=1
#pragma kernel TensorToTexture3DLUT_NHWC SUFFIX=3DLUT APPLY_3D_LUT=1 CHANNELS_FIRST=0
#pragma kernel TensorToTexture3DLUT_NCHW SUFFIX=3DLUT APPLY_3D_LUT=1 CHANNELS_FIRST=1
#pragma kernel Border2D_NHWC CHANNELS_FIRST=0
#pragma kernel Border2D_NCHW CHANNELS_FIRST=1
#pragma kernel Pad2DEdge_NHWC CHANNELS_FIRST=0
#pragma kernel Pad2DEdge_NCHW CHANNELS_FIRST=1
#pragma kernel Pad2DReflect_NHWC CHANNELS_FIRST=0
#pragma kernel Pad2DReflect_NCHW CHANNELS_FIRST=1
#pragma kernel Pad2DSymmetric_NHWC CHANNELS_FIRST=0
#pragma kernel Pad2DSymmetric_NCHW CHANNELS_FIRST=1
#pragma kernel StridedSlice_NHWC CHANNELS_FIRST=0
#pragma kernel StridedSlice_NCHW CHANNELS_FIRST=1
#pragma kernel Gather_NHWC CHANNELS_FIRST=0
#pragma kernel Gather_NCHW CHANNELS_FIRST=1

#include "Tensor.cginc"
#include "Random.cginc"

#if CHANNELS_FIRST
    #define FUNC_NAME(KERNEL, SUFFIX) KERNEL##SUFFIX##_NCHW
#else
    #define FUNC_NAME(KERNEL, SUFFIX) KERNEL##SUFFIX##_NHWC
#endif

TENSOR_DECL(X)
TENSOR_DECL(W)
TENSOR_DECL(K)
TENSOR_DECL(B)
TENSOR_DECL_RW(O)

uint4 _Pad;
uint4 _Pool;
uint4 _Stride;
uint4 _ChannelWriteMask;
float _Alpha;
float _Beta;
float _Epsilon;
float _Seed;
int _IsFirstDispatch;

[numthreads(8,8,1)]
void KERNEL_FUNC(Dense)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.flatWidth, O.flatHeight, 1);
    TENSOR_ARGS4(X, W, B, O);

    uint x = dispatchThreadID.x;
    uint y = dispatchThreadID.y;

    if (x >= O.GetFlatWidth()) return;
    if (y >= O.GetFlatHeight()) return;

    float acc = B.FastGet(x);
    for (uint i = 0; i < X.GetFlatWidth(); ++i)
        acc += X.Get(y, i) * W.Get(i, x);

    O.SetWithActivation(y, x, acc);
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Relu)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = 0.5f * (v + abs(v));

        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(PRelu)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.channels, O.width, O.height);
	TENSOR_ARGS3(X, W, O);

	uint c = dispatchThreadID.x;
	uint x = dispatchThreadID.y;
	uint y = dispatchThreadID.z;

	if (c >= O.channels) return;
	if (x >= O.width) return;
	if (y >= O.height) return;

	for (uint n = 0; n < X.batch; ++n)
	{
		float v = X.Get(n, y, x, c);
		float slope = W.BroadcastGet(n, y, x, c);

		v = max(0.0f,v) + slope * min(0.0f,v);
		O.Set(n, y, x, c, v);
	}
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(Selu)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.channels, O.width, O.height);
	TENSOR_ARGS2(X, O);

	uint c = dispatchThreadID.x;
	uint x = dispatchThreadID.y;
	uint y = dispatchThreadID.z;

	if (c >= O.channels) return;
	if (x >= O.width) return;
	if (y >= O.height) return;

	for (uint n = 0; n < X.batch; ++n)
	{
		float v = X.Get(n, y, x, c);
		v = _Beta * (max(v, 0.0f) + min(_Alpha * (exp(v) - 1.0f), 0.0f));

		O.Set(n, y, x, c, v);
	}
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Neg)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = -v;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(Reciprocal)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.channels, O.width, O.height);
	TENSOR_ARGS2(X, O);

	uint c = dispatchThreadID.x;
	uint x = dispatchThreadID.y;
	uint y = dispatchThreadID.z;

	if (c >= O.channels) return;
	if (x >= O.width) return;
	if (y >= O.height) return;

	for (uint n = 0; n < X.batch; ++n)
	{
		float v = X.Get(n, y, x, c);
		v = 1.0f / v;
		O.Set(n, y, x, c, v);
	}
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Swish)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = v / (1 + exp(-v));
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Tanh)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = tanh(clamp(v,-16.0f,16.0f));//clamp to avoid NaNs for large values.

        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Sigmoid)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = 1 / (1 + exp(-v));
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Relu6)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = min(max(0, v), 6);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Elu)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        if (v <= 0)
            v = _Alpha * (exp(v) - 1);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(LeakyRelu)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = max(v, _Alpha * v);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Exp)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = exp(v);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Log)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = log(v);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(Sqrt)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.channels, O.width, O.height);
	TENSOR_ARGS2(X, O);

	uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
	if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

	for (uint n = 0; n < X.batch; ++n)
	{
		float v = X.Get(n, y, x, c);
		v = sqrt(v);
		O.Set(n, y, x, c, v);
	}
}

float signed_pow(float f, float e)
{
    // handle negative f
    float v = pow(abs(f), e);
    float s = (e % 2 == 1) ?
        sign(f):    // exponent is odd  => sign(f) * pow(abs(f), e)
        1;            // exponent is even => pow(abs(f), e)
    return v * s;
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Pow)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = signed_pow(v, _Alpha);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(Clip)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.channels, O.width, O.height);
	TENSOR_ARGS2(X, O);

	uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
	if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

	for (uint n = 0; n < X.batch; ++n)
	{
		float v = X.Get(n, y, x, c);
		v = clamp(v, _Alpha, _Beta);
		O.Set(n, y, x, c, v);
	}
}

[numthreads(4,4,4)]
void KERNEL_FUNC(BroadcastAdd)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v =
            X.BroadcastGet(n, y, x, c) +
            B.BroadcastGet(n, y, x, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(BroadcastSub)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v =
            X.BroadcastGet(n, y, x, c) -
            B.BroadcastGet(n, y, x, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(BroadcastMul)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float v =
            X.BroadcastGet(n, y, x, c) *
            B.BroadcastGet(n, y, x, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(BroadcastDiv)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v =
            X.BroadcastGet(n, y, x, c) /
            B.BroadcastGet(n, y, x, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(BroadcastPow)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = signed_pow(
            X.BroadcastGet(n, y, x, c),
            B.BroadcastGet(n, y, x, c));
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(BroadcastMin)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = min(
            X.BroadcastGet(n, y, x, c),
            B.BroadcastGet(n, y, x, c));
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(BroadcastMax)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = max(
            X.BroadcastGet(n, y, x, c),
            B.BroadcastGet(n, y, x, c));
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(BroadcastMean)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        a *= _IsFirstDispatch ? _Alpha : 1.0f;
        float b = B.BroadcastGet(n, y, x, c) * _Alpha;
        float v = a + b;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(BroadcastGreater)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        float b = B.BroadcastGet(n, y, x, c);
        float v = (a > b) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(BroadcastGreaterEqual)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        float b = B.BroadcastGet(n, y, x, c);
        float v = (a >= b) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(BroadcastLess)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        float b = B.BroadcastGet(n, y, x, c);
        float v = (a < b) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(BroadcastLessEqual)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        float b = B.BroadcastGet(n, y, x, c);
        float v = (a <= b) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(BroadcastEqual)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = X.BroadcastGet(n, y, x, c);
        float b = B.BroadcastGet(n, y, x, c);
        float v = (a == b) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(BroadcastLogicalOr)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = (X.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f: 1.0f;
        float b = (B.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f: 1.0f;
        float v = ((a + b) >= 1.0f) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(BroadcastLogicalAnd)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = (X.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f : 1.0f;
        float b = (B.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f : 1.0f;
        float v = ((a + b) > 1.5f) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(BroadcastLogicalXor)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_TWOINPUTS(X, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;
    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float a = (X.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f : 1.0f;
        float b = (B.BroadcastGet(n, y, x, c) == 0.0f) ? 0.0f : 1.0f;
        float v = ((a + b) == 1.0f) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(LogicalNot)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels)
        return;
    if (x >= O.width)
        return;
    if (y >= O.height)
        return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = (X.Get(n, y, x, c) == 0.0f) ? 1.0f : 0.0f;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,1)]
void KERNEL_FUNC(ReduceMin)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.width, O.height, 1);
    TENSOR_ARGS3(X, B, O);

    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float minV = FLT_MAX;
        for (uint c = 0; c < X.channels; ++c)
        {
            float v = X.Get(n, y, x, c);
            minV = min(v, minV);
        }
        O.Set(n, y, x, 0, minV);
    }
}

[numthreads(4,4,1)]
void KERNEL_FUNC(ReduceMax)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.width, O.height, 1);
    TENSOR_ARGS3(X, B, O);

    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float maxV = -FLT_MAX;
        for (uint c = 0; c < X.channels; ++c)
        {
            float v = X.Get(n, y, x, c);
            maxV = max(v, maxV);
        }
        O.Set(n, y, x, 0, maxV);
    }
}

[numthreads(4,4,1)]
void KERNEL_FUNC(ReduceSum)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.width, O.height, 1);
    TENSOR_ARGS3(X, B, O);

    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 0;
        for (uint c = 0; c < X.channels; ++c)
            v += X.Get(n, y, x, c);
        O.Set(n, y, x, 0, v);
    }
}

[numthreads(4,4,1)]
void KERNEL_FUNC(ReduceMean)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.width, O.height, 1);
    TENSOR_ARGS3(X, B, O);

    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 0;
        for (uint c = 0; c < X.channels; ++c)
            v += X.Get(n, y, x, c);

        v /= X.channels;
        O.Set(n, y, x, 0, v);
    }
}

[numthreads(4,4,1)]
void KERNEL_FUNC(ReduceProd)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.width, O.height, 1);
    TENSOR_ARGS3(X, B, O);

    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 1;
        for (uint c = 0; c < X.channels; ++c)
            v *= X.Get(n, y, x, c);
        O.Set(n, y, x, 0, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Copy)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    // NOTE: dispatched over X (not O)
    DISPATCH_ARGS(X.channels, X.width, X.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= X.channels) return;    if (x >= X.width) return;       if (y >= X.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        O.Set(n + _Pad[0], y + _Pad[1], x + _Pad[2], c + _Pad[3], v);
    }
}

[numthreads(4,4,4)]
void ReshapeFromNHWCModel_NCHW(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        //find the memory offset of target item in HWC format
        uint index_NHWC = O.IndexHWC(n,y,x,c);
        //from this offset find indices of item in HWC format before the reshape
        uint c_NHWC = index_NHWC % X.channels;
        uint x_NHWC = (index_NHWC / X.channels) % X.width;
        uint y_NHWC = (index_NHWC / (X.channels * X.width)) % X.height;
        uint b_NHWC = (index_NHWC / (X.channels * X.width * X.height)) % X.batch;

        //finally copy item
        float v = X.Get(b_NHWC, y_NHWC, x_NHWC, c_NHWC);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Dropout)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;    uint x = dispatchThreadID.y;    uint y = dispatchThreadID.z;
    if (c >= O.channels) return;    if (x >= O.width) return;       if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float4 seed = float4(n / O.batch, y / O.height, x / O.width, c / O.channels);
        seed = frac(seed + _Seed);

        float v = X.Get(n, y, x, c);
        v *= Bernoulli(seed, 1 - _Alpha) / (1 - _Alpha);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(ScaleBias)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS4(X, W, B, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    float scale = W.Get(0, 0, 0, c);
    float bias = B.Get(0, 0, 0, c);

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = X.Get(n, y, x, c);
        v = v * scale + bias;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(16,4,1)]
void KERNEL_FUNC(Softmax)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.flatWidth, O.flatHeight, 1);
    TENSOR_ARGS2(X, O);

    uint x = dispatchThreadID.x;
    uint y = dispatchThreadID.y;

    if (x >= O.GetFlatWidth()) return;
    if (y >= O.GetFlatHeight()) return;

    float maxV = -FLT_MAX;
    uint i;
    for (i = 0; i < X.GetFlatWidth(); ++i)
    {
        float v = X.Get(y, i);
        if (v > maxV)
            maxV = v;
    }

    float acc = 0.0f;
    for (i = 0; i < X.GetFlatWidth(); ++i)
    {
        float v = X.Get(y, i);
        acc += exp(v - maxV);
    }

    float v = X.Get(y, x);
    v = exp(v - maxV) / acc;
    O.Set(y, x, v);
}

[numthreads(16, 4, 1)]
void KERNEL_FUNC(LogSoftmax)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
	DISPATCH_ARGS(O.flatWidth, O.flatHeight, 1);
	TENSOR_ARGS2(X, O);

	uint x = dispatchThreadID.x;
	uint y = dispatchThreadID.y;

	if (x >= O.GetFlatWidth()) return;
	if (y >= O.GetFlatHeight()) return;

	float maxV = -FLT_MAX;
	uint i;
	for (i = 0; i < X.GetFlatWidth(); ++i)
	{
		float v = X.Get(y, i);
		if (v > maxV)
			maxV = v;
	}

	float acc = 0.0f;
	for (i = 0; i < X.GetFlatWidth(); ++i)
	{
		float v = X.Get(y, i);
		acc += exp(v - maxV);
	}

	float v = X.Get(y, x);
	v = log(exp(v - maxV) / acc);
	O.Set(y, x, v);
}


[numthreads(4,4,4)]
void KERNEL_FUNC(Upsample2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    // NOTE: dispatched over X (not O)
    DISPATCH_ARGS(X.channels, X.width, X.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= X.channels) return;
    if (x >= X.width) return;
    if (y >= X.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float v = X.Get(n, y, x, c);

        for (uint dy = 0; dy < _Pool.y; ++dy)
            for (uint dx = 0; dx < _Pool.x; ++dx)
            {
                uint oy = y * _Pool.y + dy;
                uint ox = x * _Pool.x + dx;
                O.Set(n, oy, ox, c, v);
            }
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(UpsampleBilinear2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    float2 dstPos = float2(x, y);
    float2 srcPos = (dstPos + 0.5) / _Pool.xy - 0.5;

    for (uint n = 0; n < O.batch; ++n)
    {
        float p00 = X.ClampGet(n, floor(srcPos) + float2(0, 0), c);
        float p01 = X.ClampGet(n, floor(srcPos) + float2(0, 1), c);
        float p10 = X.ClampGet(n, floor(srcPos) + float2(1, 0), c);
        float p11 = X.ClampGet(n, floor(srcPos) + float2(1, 1), c);

        float v =
            p00 * (1-frac(srcPos.x)) * (1-frac(srcPos.y)) +
            p01 * (1-frac(srcPos.x)) *    frac(srcPos.y)  +
            p10 *    frac(srcPos.x)  * (1-frac(srcPos.y)) +
            p11 *    frac(srcPos.x)  *    frac(srcPos.y);

        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(MaxPool2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float maxV = -FLT_MAX;
        for (uint dy = 0; dy < _Pool.y; ++dy)
            for (uint dx = 0; dx < _Pool.x; ++dx)
            {
                uint2 pos = uint2(x, y) * _Stride.xy + uint2(dx, dy);
                float v = X.SafeGet(n, pos, c, _Pad.xy, -FLT_MAX );
                maxV = max(v, maxV);
            }

        O.Set(n, y, x, c, maxV);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(AvgPool2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    uint2 leftCorner = _Pad.xy;
    uint2 rightCorner = uint2(X.width, X.height) + _Pad.xy;
    for (uint n = 0; n < X.batch; ++n)
    {
        float acc = 0;
        float counter = 0;
        for (uint dy = 0; dy < _Pool.y; ++dy)
            for (uint dx = 0; dx < _Pool.x; ++dx)
            {
                uint2 pos = uint2(x, y) * _Stride.xy + uint2(dx, dy);

                bool mask = all(pos >= leftCorner) && all(pos < rightCorner);
                acc += (mask)? X.Get(n, pos.y - leftCorner.y, pos.x - leftCorner.x, c): 0;
                counter += (mask)? 1: 0;
            }

        acc /= counter;
        O.Set(n, y, x, c, acc);
    }
}

[numthreads(32,1,1)]
void KERNEL_FUNC(GlobalMaxPool2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    if (c >= O.channels) return;
    //ASSERT(X.batch == O.batch)

    for (uint n = 0; n < X.batch; ++n)
    {
        float maxV = -FLT_MAX;
        for (uint y = 0; y < X.height; ++y)
            for (uint x = 0; x < X.width; ++x)
            {
                float v = X.Get(n, y, x, c);
                maxV = max(v, maxV);
            }

        O.Set(n, 0, 0, c, maxV);
    }
}

[numthreads(32,1,1)]
void KERNEL_FUNC(GlobalAvgPool2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    if (c >= O.channels) return;
    //ASSERT(X.batch == O.batch)

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 0;
        for (uint y = 0; y < X.height; ++y)
            for (uint x = 0; x < X.width; ++x)
                v += X.Get(n, y, x, c);

        v /= (X.height * X.width);
        O.Set(n, 0, 0, c, v);
    }
}


[numthreads(32, 1, 1)]
void KERNEL_FUNC(GlobalAvgVariancePool2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    if (c >= O.channels) return;
    //ASSERT(X.batch == O.batch)

    for (uint n = 0; n < X.batch; ++n)
    {
	    float mean = 0;
	    float mean2 = 0;
	    for (uint y = 0; y < X.height; ++y)
	    {
		    for (uint x = 0; x < X.width; ++x)
		    {
			    float v = X.Get(n, y, x, c);
			    mean += v;
			    mean2 += v * v;
		    }
	    }

	    mean /= (X.height * X.width);
	    mean2 /= (X.height * X.width);

	    O.Set(n, 0, 0, c, mean);
	    O.Set(n, 1, 0, c, mean2 - mean * mean);
    }
}

[numthreads(32,1,1)]
void KERNEL_FUNC(InstanceNorm)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, 1, 1);
    TENSOR_ARGS4(X, W, B, O);

    uint c = dispatchThreadID.x;
    if (c >= O.channels) return;
    //ASSERT(X.shape == O.shape)

    float gamma = W.Get(0, 0, 0, c);
    float beta = B.Get(0, 0, 0, c);

    // There are 2 sources of numerical errors when computing Variance over large number of elements:
    // 1) summing N floating point numbers in sequence has a worst-case error that grows proportional to N
    // 2) because SumSq and (SumÃ—Sum)/N can be very similar numbers, cancellation can lead to the precision of the result
    //    to be much less than the inherent precision of the floating-point arithmetic used to perform the computation.
    //    This is particularly bad if the standard deviation is small relative to the mean!
    // Below algorithm is improved by adopting the method of the assumed mean and Neumaier compensated summation
    // see: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
    // see: https://en.wikipedia.org/wiki/Kahan_summation_algorithm

    for (uint n = 0; n < O.batch; ++n)
    {
        uint i;
        uint count = O.height * O.width;

        // estimate mean, result is approximate due to litimited floating point precision
        // however it is good enough for the following calculation of variance over the shifted data
        float approximateMean = X.Get(n, 0, c);
            {
            float sum = 0;
            for (i = 0; i < count; ++i)
            {
                float delta = X.Get(n, i, c) - approximateMean;
                sum += delta;
            }
            approximateMean += sum / count;
            }

        // compute mean & variance
        // to improve precision, variance over shifted data is cacluated: Var(X - K) = Var(X)
        // estimated mean is used instead of 1st element to make reference impl more stable in respect to the order of the elements
        // see: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
        // K   <- approximateMean
        // Ex  <- sum
        // Ex2 <- sumSq

        float sum = 0, sumSq = 0;
        float correction = 0, correctionSq = 0;
        for (i = 0; i < count; ++i)
        {
            float delta = X.Get(n, i, c) - approximateMean;
            sum = neumaierAdd(sum, delta, correction);
            sumSq = neumaierAdd(sumSq, delta * delta, correctionSq);
        }
        sum += correction;
        sumSq += correctionSq;

        float mean = approximateMean + sum / count;
        float var = (sumSq - (sum * sum) / count) / count;

        // apply normalization
        for (uint j = 0; j < count; ++j)
            {
            float v = X.Get(n, j, c);
            v = gamma * (v - mean) / sqrt(var + _Epsilon) + beta;
            O.SetWithActivation(n, j, c, v);
            }
    }
}

// https://github.com/andravin/wincnn
// https://arxiv.org/pdf/1509.09308.pdf
// Winograd: 4x4 image, 3x3 kernel, 2x2 output
static const float4x4 Winograd_BT = float4x4(float4(1, 0, -1, 0), float4(0, 1, 1, 0), float4(0, -1, 1, 0), float4(0, -1, 0, 1));
static const float4x4 Winograd_B = transpose(Winograd_BT);

static const float4x3 Winograd_G = float4x3(float3(1, 0, 0), float3(0.5, 0.5, 0.5), float3(0.5, -0.5, 0.5), float3(0, 0, 1));
static const float3x4 Winograd_GT = transpose(Winograd_G);

static const float2x4 Winograd_AT = float2x4(float4(1, 1, 1, 0), float4(0, 1, -1, 1));
static const float4x2 Winograd_A = transpose(Winograd_AT);


[numthreads(64, 1, 1)]
void KERNEL_FUNC(Conv2DWinograd_2x2_3x3)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_ARGS4(X, K, B, O);

    uint k = dispatchThreadID.x;
    if (k >= K.channels) return;

    uint2 index = 2 * dispatchThreadID.yz;

    uint2 pad = uint2(_Pad[0], _Pad[1]);
    uint2 XDim = uint2(X.width, X.height);

    for (uint n = 0; n < O.batch; ++n)
    {
        float2x2 acc = B.FastGet(k);

        for (uint c = 0; c < X.channels; ++c)
        {
            // 16 loads per thread
            float4x4 d;
            d[0][0] = any(index.xy + uint2(0, 0) >= XDim + pad) || any(index.xy + uint2(0, 0) < pad) ? 0.0 : X.Get(n, index.xy + uint2(0, 0) - pad, c);
            d[0][1] = any(index.xy + uint2(1, 0) >= XDim + pad) || any(index.xy + uint2(1, 0) < pad) ? 0.0 : X.Get(n, index.xy + uint2(1, 0) - pad, c);
            d[0][2] = any(index.xy + uint2(2, 0) >= XDim + pad) || any(index.xy + uint2(2, 0) < pad) ? 0.0 : X.Get(n, index.xy + uint2(2, 0) - pad, c);
            d[0][3] = any(index.xy + uint2(3, 0) >= XDim + pad) || any(index.xy + uint2(3, 0) < pad) ? 0.0 : X.Get(n, index.xy + uint2(3, 0) - pad, c);
            d[1][0] = any(index.xy + uint2(0, 1) >= XDim + pad) || any(index.xy + uint2(0, 1) < pad) ? 0.0 : X.Get(n, index.xy + uint2(0, 1) - pad, c);
            d[1][1] = any(index.xy + uint2(1, 1) >= XDim + pad) || any(index.xy + uint2(1, 1) < pad) ? 0.0 : X.Get(n, index.xy + uint2(1, 1) - pad, c);
            d[1][2] = any(index.xy + uint2(2, 1) >= XDim + pad) || any(index.xy + uint2(2, 1) < pad) ? 0.0 : X.Get(n, index.xy + uint2(2, 1) - pad, c);
            d[1][3] = any(index.xy + uint2(3, 1) >= XDim + pad) || any(index.xy + uint2(3, 1) < pad) ? 0.0 : X.Get(n, index.xy + uint2(3, 1) - pad, c);
            d[2][0] = any(index.xy + uint2(0, 2) >= XDim + pad) || any(index.xy + uint2(0, 2) < pad) ? 0.0 : X.Get(n, index.xy + uint2(0, 2) - pad, c);
            d[2][1] = any(index.xy + uint2(1, 2) >= XDim + pad) || any(index.xy + uint2(1, 2) < pad) ? 0.0 : X.Get(n, index.xy + uint2(1, 2) - pad, c);
            d[2][2] = any(index.xy + uint2(2, 2) >= XDim + pad) || any(index.xy + uint2(2, 2) < pad) ? 0.0 : X.Get(n, index.xy + uint2(2, 2) - pad, c);
            d[2][3] = any(index.xy + uint2(3, 2) >= XDim + pad) || any(index.xy + uint2(3, 2) < pad) ? 0.0 : X.Get(n, index.xy + uint2(3, 2) - pad, c);
            d[3][0] = any(index.xy + uint2(0, 3) >= XDim + pad) || any(index.xy + uint2(0, 3) < pad) ? 0.0 : X.Get(n, index.xy + uint2(0, 3) - pad, c);
            d[3][1] = any(index.xy + uint2(1, 3) >= XDim + pad) || any(index.xy + uint2(1, 3) < pad) ? 0.0 : X.Get(n, index.xy + uint2(1, 3) - pad, c);
            d[3][2] = any(index.xy + uint2(2, 3) >= XDim + pad) || any(index.xy + uint2(2, 3) < pad) ? 0.0 : X.Get(n, index.xy + uint2(2, 3) - pad, c);
            d[3][3] = any(index.xy + uint2(3, 3) >= XDim + pad) || any(index.xy + uint2(3, 3) < pad) ? 0.0 : X.Get(n, index.xy + uint2(3, 3) - pad, c);

            float3x3 g;
            g[0][0] = K.Get(0, 0, c, k);
            g[0][1] = K.Get(0, 1, c, k);
            g[0][2] = K.Get(0, 2, c, k);
            g[1][0] = K.Get(1, 0, c, k);
            g[1][1] = K.Get(1, 1, c, k);
            g[1][2] = K.Get(1, 2, c, k);
            g[2][0] = K.Get(2, 0, c, k);
            g[2][1] = K.Get(2, 1, c, k);
            g[2][2] = K.Get(2, 2, c, k);

            float4x4 v = mul(Winograd_G,  mul(g, Winograd_GT));
            float4x4 u = mul(Winograd_BT, mul(d, Winograd_B));
            float2x2 y = mul(Winograd_AT, mul(v*u, Winograd_A));

            acc += y;
        }

        // 4 writes per thread
        if (index.y < O.height && index.x < O.width)
        O.SetWithActivation(n, index.y + 0, index.x + 0, k, acc[0][0]);
        if (index.y + 1 < O.height && index.x < O.width)
        O.SetWithActivation(n, index.y + 1, index.x + 0, k, acc[1][0]);
        if (index.y < O.height && index.x + 1 < O.width)
        O.SetWithActivation(n, index.y + 0, index.x + 1, k, acc[0][1]);
        if (index.y + 1 < O.height && index.x + 1 < O.width)
        O.SetWithActivation(n, index.y + 1, index.x + 1, k, acc[1][1]);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Conv2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_ARGS4(X, K, B, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = B.FastGet(k);
        for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
        {
            for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
            {
                uint2 pos = uint2(x, y) * _Stride.xy + uint2(dx, dy);
                for (uint c = 0; c < X.channels; ++c)
                {
                    float v = X.SafeGet(n, pos, c, _Pad.xy);
                    acc += v * K.Get(dy, dx, c, k);
                }
            }
        }

        O.SetWithActivation(n, y, x, k, acc);
    }
}

NUMTHREADS((16,4,4), (8,4,4), (4,4,4))
void KERNEL_FUNC(DepthwiseConv2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_ARGS4(X, K, B, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = B.FastGet(k);
        for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
            for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
            {
                uint2 pos = uint2(x, y) * _Stride.xy + uint2(dx, dy);
                float v = X.SafeGet(n, pos, k, _Pad.xy);
                acc += v * K.Get(dy, dx, 0, k);
            }

        O.Set(n, y, x, k, acc);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Unstride2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < O.batch; ++n)
    {
        int xx = (int)x - (int)_Pad.x;
        int yy = (int)y - (int)_Pad.y;

        int my = yy % _Stride.y;
        int mx = xx % _Stride.x;

        int oy = yy / _Stride.y;
        int ox = xx / _Stride.x;

        bool mask = ox >= 0 && oy >= 0 && ox < (int)X.width && oy < (int)X.height &&
            my == 0 && mx == 0;

        float v = mask ? X.Get(n, (uint)oy, (uint)ox, c) : 0;
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4,4,4)]
void KERNEL_FUNC(Conv2DTrans)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_ARGS4(X, K, B, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    uint2 strideMask = _Stride.xy - 1;

    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = B.FastGet(k);
        for (uint dy = (y + _Pad.y) & strideMask.y; dy < K.GetKernelHeight(); dy += _Stride.y)
        {
            for (uint dx = (x + _Pad.x) & strideMask.x; dx < K.GetKernelWidth(); dx += _Stride.x)
            {
                for (uint c = 0; c < X.channels; ++c)
                {
                    uint xx = x + dx;
                    uint yy = y + dy;

                    uint oy = (yy - _Pad.y) / _Stride.y;
                    uint ox = (xx - _Pad.x) / _Stride.x;

                    // early out if read input index fall upon leftmost outer zero padding
                    if ((xx - _Pad.x) < 0) continue;
                    if ((yy - _Pad.y) < 0) continue;

                    // early out if read input index fall upon rightmost outer zero padding
                    if (ox >= X.width) continue;
                    if (oy >= X.height) continue;

                    acc += X.Get(n, oy, ox, c) * K.Get(K.GetKernelHeight() - 1 - dy, K.GetKernelWidth() - 1 - dx, c, k);
                }
            }
        }

        O.Set(n, y, x, k, acc);
    }
}


Texture2D<float4> Xtex2D;
Texture3D<float4> Xtex3D;
Texture2DArray<float4> Xtex2DArray;
SamplerState samplerXtex2D { Filter = MIN_MAG_LINEAR_MIP_POINT; AddressU = Clamp; AddressV = Clamp; };
SamplerState samplerXtex3D { Filter = MIN_MAG_LINEAR_MIP_POINT; AddressU = Clamp; AddressV = Clamp; AddressW = Clamp; };
SamplerState samplerXtex2DArray { Filter = MIN_MAG_LINEAR_MIP_POINT; AddressU = Clamp; AddressV = Clamp; };

RWTexture2D<float4> Otex2D;
RWTexture3D<float4> Otex3D;
RWTexture2DArray<float4> Otex2DArray;

float4 _Scale;
float4 _Bias;
float2 _LutParams;
bool _FlipY;

// TODO: call TextureToTensor(v, dispatchThreadID) from Tex2DToTensor() { v = Xtex2D.SampleLevel }
[numthreads(8,8,1)]
void KERNEL_FUNC(TextureToTensor)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    TENSOR_ARG_RW(O);

    uint b = _Pad.x;
    uint x = dispatchThreadID.x + _Pad.y;
    uint y = dispatchThreadID.y + _Pad.z;
    uint c = dispatchThreadID.z + _Pad.w;

    if (y >= O.height || x >= O.width)
        return;

    // calculate texture coordinates:
    //  offset by 0.5 to get texel centers
    //  divide by texture resolution (_Pool)
    float3 uvw = (float3)dispatchThreadID + float3(0.5f, 0.5f, 0);
    uvw.xy /= _Pool.xy;
    if (_FlipY)
        uvw.y = 1 - uvw.y;

    float4 v = Xtex2D.SampleLevel(samplerXtex2D, uvw.xy, 0);
    //texArray.SampleLevel(smpArray, loc, 0);

    bool specialCaseWhenChannelMaskIsEmptyStoresAverage = true;
    for (int i = 0; i < 4; ++i)
    {
        if (_ChannelWriteMask[i] == 1)
        {
            O.Set(b, y, x, c, v[i]);
            c += 1;
            specialCaseWhenChannelMaskIsEmptyStoresAverage = false;
        }
    }

    if (specialCaseWhenChannelMaskIsEmptyStoresAverage)
    {
        float avg = (v.r + v.g + v.b) / 3.0f;
        O.Set(b, y, x, c, avg);
    }
}

[numthreads(8,8,1)]
void FUNC_NAME(TensorToTexture,SUFFIX)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    TENSOR_ARG(X);

    uint b = _Pad.x;
    uint x = dispatchThreadID.x + _Pad.y;
    uint y = dispatchThreadID.y + _Pad.z;
    uint c = dispatchThreadID.z + _Pad.w;

    if (y >= X.height || x >= X.width)
        return;

    if (_FlipY)
        y = X.height - 1 - y;

    float4 v = 0;

    int channelRemainder = X.channels - c;
    if (channelRemainder == 1)
    {
        // broadcast to all channels
        v = _Scale.x * X.Get(b, y, x, c) + _Bias.x;
    }
    else if (channelRemainder == 2)
    {
        v.r = _Scale.x * X.Get(b, y, x, c+0) + _Bias.x;
        v.g = _Scale.y * X.Get(b, y, x, c+1) + _Bias.y;
        v.b = 0;
        v.a = 1;
    }
    else if (channelRemainder == 3)
    {
        v.r = _Scale.x * X.Get(b, y, x, c+0) + _Bias.x;
        v.g = _Scale.y * X.Get(b, y, x, c+1) + _Bias.y;
        v.b = _Scale.z * X.Get(b, y, x, c+2) + _Bias.z;
        v.a = 1;
    }
    else if (channelRemainder >= 4)
    {
        v.r = _Scale.x * X.Get(b, y, x, c+0) + _Bias.x;
        v.g = _Scale.y * X.Get(b, y, x, c+1) + _Bias.y;
        v.b = _Scale.z * X.Get(b, y, x, c+2) + _Bias.z;
        v.a = _Scale.w * X.Get(b, y, x, c+3) + _Bias.w;
    }

    #if APPLY_3D_LUT
        float3 uvw = v.xyz * _LutParams.yyy * _LutParams.xxx + _LutParams.xxx * 0.5f;
        v.xyz = Xtex3D.SampleLevel(samplerXtex3D, uvw, 0).xyz;
    #endif

    Otex2D[dispatchThreadID.xy] = v;
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(Border2D)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    // NOTE: negative "pad" variable crop X tensor
    int croppedWidth = _Pool.x;
    int croppedHeight = _Pool.y;
    int readX = x - _Pad.x;
    int readY = y - _Pad.y;

    for (uint n = 0; n < O.batch; ++n)
    {
        float v;
        if (readX < 0 || readX >= croppedWidth ||
            readY < 0 || readY >= croppedHeight)
        {
            v = _Beta;
        }
        else
        {
            v = X.Get(n, readY, readX, c);
        }
        O.Set(n, y, x, c, v);
    }
}

void ClampHWToTensorShape(uint2 Xshape, inout int height, inout int width)
{
    width = max(width, 0);
    height = max(height, 0);
    width = min(width, (int)Xshape.x - 1);
    height = min(height, (int)Xshape.y - 1);
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(Pad2DEdge)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    int readX = x - _Pad.x;
    int readY = y - _Pad.y;
    uint2 Xshape = uint2(X.width, X.height);

    //clamp read indices to source
    ClampHWToTensorShape(Xshape, readY, readX);

    for (uint n = 0; n < O.batch; ++n)
    {
        float v = X.Get(n, readY, readX, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(Pad2DReflect)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    int readX = x - _Pad.x;
    int readY = y - _Pad.y;
    uint2 Xshape = uint2(X.width, X.height);

    int lastXIndex = Xshape.x - 1;
    int lastYIndex = Xshape.y - 1;

    //x reflect indexing
    if (readX < 0)
        readX = -readX;
    else if (readX > lastXIndex)
        readX = lastXIndex - (readX - lastXIndex);

    //y reflect indexing
    if (readY < 0)
        readY = -readY;
    else if (readY > lastYIndex)
        readY = lastYIndex - (readY - lastYIndex);

    //clamp read indices to source
    ClampHWToTensorShape(Xshape, readY, readX);

    for (uint n = 0; n < O.batch; ++n)
    {
        float v = X.Get(n, readY, readX, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(Pad2DSymmetric)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    int readX = x - _Pad.x;
    int readY = y - _Pad.y;
    uint2 Xshape = uint2(X.width, X.height);

    int lastXIndex = Xshape.x - 1;
    int lastYIndex = Xshape.y - 1;

    //x symmetric indexing
    if (readX < 0)
        readX = -readX - 1;
    else if (readX > lastXIndex)
        readX = lastXIndex - (readX - lastXIndex) + 1;

    //y symmetric indexing
    if (readY < 0)
        readY = -readY - 1;
    else if (readY > lastYIndex)
        readY = lastYIndex - (readY - lastYIndex) + 1;

    //clamp read indices to source
    ClampHWToTensorShape(Xshape, readY, readX);

    for (uint n = 0; n < O.batch; ++n)
    {
        float v = X.Get(n, readY, readX, c);
        O.Set(n, y, x, c, v);
    }
}

[numthreads(4, 4, 4)]
void KERNEL_FUNC(StridedSlice)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS2(X, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint b = 0; b < O.batch; ++b)
    {
        float v = X.Get(_Pad.x + b * _Stride.x,
                        _Pad.y + y * _Stride.y,
                        _Pad.z + x * _Stride.z,
                        _Pad.w + c * _Stride.w);
        O.Set(b, y, x, c, v);
    }
}

uint _Axis;

[numthreads(4, 4, 4)]
void KERNEL_FUNC(Gather)(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(O.channels, O.width, O.height);
    TENSOR_ARGS3(X, K, O);

    uint c = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (c >= O.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    for (uint n = 0; n < X.batch; ++n)
    {
        float v = 0.0;
        if (_Axis == 0)
            v = X.Get((uint)K.FastGet(n), y, x, c);
        else if (_Axis == 1)
            v = X.Get(n, (uint)K.FastGet(y), x, c);
        else if (_Axis == 2)
            v = X.Get(n, y, (uint)K.FastGet(x), c);
        else
            v = X.Get(n, y, x, (uint)K.FastGet(c));

        O.Set(n, y, x, c, v);
    }
}
